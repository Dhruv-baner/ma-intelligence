{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e6121dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Web requests and data handling\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Date and time utilities\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "# File handling\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Adding our src directory to Python path so we can import our custom functions later\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Displaying settings for better notebook output\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d299769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We need to test the connection to the SEC EDGAR API Connection...It's a free and open database\n",
      "----------------------------------------------------------------------------------------------------\n",
      "üîå Testing connection to SEC EDGAR database...\n",
      "--------------------------------------------------\n",
      "üì° Attempting to connect to SEC EDGAR...\n",
      "Connected to the SEC EDGAR database\n",
      "Retrieved data for 10069 companies\n",
      "Response time: 0.42 seconds\n",
      "\n",
      "üè¢ Sample companies from SEC database:\n",
      "   ‚Ä¢ NVDA: NVIDIA CORP\n",
      "   ‚Ä¢ MSFT: MICROSOFT CORP\n",
      "   ‚Ä¢ AAPL: Apple Inc.\n",
      "   ‚Ä¢ GOOGL: Alphabet Inc.\n",
      "   ‚Ä¢ AMZN: AMAZON COM INC\n",
      "\n",
      "üéØ SEC API is working! We can access 10069 companies.\n",
      "\n",
      "==================================================\n",
      "üîÑ Connection test complete. Ready for next step...\n"
     ]
    }
   ],
   "source": [
    "print (\"We need to test the connection to the SEC EDGAR API Connection...It's a free and open database\"  )\n",
    "print(\"-\" * 100)\n",
    "\n",
    "\n",
    "\n",
    "# Cell 2: Test SEC EDGAR API Connection\n",
    "print(\"üîå Testing connection to SEC EDGAR database...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# SEC requires us to identify ourselves - this is mandatory!\n",
    "headers = {\n",
    "    'User-Agent': 'M&A Intelligence Platform (dhruvb363@gmail.com.com)'\n",
    "}\n",
    "\n",
    "# Test with a simple API endpoint - get list of companies\n",
    "test_url = \"https://www.sec.gov/files/company_tickers.json\"\n",
    "\n",
    "try:\n",
    "    print(\"üì° Attempting to connect to SEC EDGAR...\")\n",
    "    \n",
    "    # Make the request with a timeout\n",
    "    response = requests.get(test_url, headers=headers, timeout=10)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        print(\"Connected to the SEC EDGAR database\")\n",
    "        \n",
    "        # Parse the JSON response\n",
    "        company_data = response.json()\n",
    "        \n",
    "        # Show some basic info about what we got\n",
    "        print(f\"Retrieved data for {len(company_data)} companies\")\n",
    "        print(f\"Response time: {response.elapsed.total_seconds():.2f} seconds\")\n",
    "        \n",
    "        # Show a few example companies to verify data quality\n",
    "        print(\"\\nüè¢ Sample companies from SEC database:\")\n",
    "        count = 0\n",
    "        for key, company in company_data.items():\n",
    "            if count < 5:  # Show first 5 companies\n",
    "                ticker = company.get('ticker', 'N/A')\n",
    "                title = company.get('title', 'N/A')\n",
    "                print(f\"   ‚Ä¢ {ticker}: {title}\")\n",
    "                count += 1\n",
    "        \n",
    "        print(f\"\\nüéØ SEC API is working! We can access {len(company_data)} companies.\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"‚ùå ERROR: Failed to connect. Status code: {response.status_code}\")\n",
    "        print(\"This might be a temporary issue. Try again in a few minutes.\")\n",
    "        \n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"‚ùå CONNECTION ERROR: {str(e)}\")\n",
    "    print(\"Check your internet connection and try again.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå UNEXPECTED ERROR: {str(e)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"üîÑ Connection test complete. Ready for next step...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75b0338",
   "metadata": {},
   "source": [
    "### **Getting the Data:** \n",
    "\n",
    "- ### I want to check out whether we can get the SEC filings, which will be crucial for our NLP tasks later in the project.     Let's run a test to check this out! \n",
    "\n",
    "- ### After that, I will use feedparser to go through a bunch of RSS news feeds, which will later help me track daily news and updates \n",
    "\n",
    "- ### I'm going to try out multiple sources at once...Eeven if one fall shorts, something will work at least\n",
    "\n",
    "- ### I'm also going to test out API's for financial data, to get information on stocks and so on. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f32e198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Looking up recent filings for Apple Inc (AAPL)...\n",
      "‚úÖ Successfully downloaded company data!\n",
      "üè¢ Company: Apple Inc.\n",
      "üìä Industry: Electronic Computers\n",
      "\n",
      "üìã Found 1007 recent filings\n",
      "\n",
      "üóÇÔ∏è Most Recent Filings:\n",
      "   üìÑ 4 filed on 2025-08-12\n",
      "   üìÑ 144 filed on 2025-08-08\n",
      "   üéØ 10-Q filed on 2025-08-01\n",
      "   üéØ 8-K filed on 2025-07-31\n",
      "   üìÑ SCHEDULE 13G/A filed on 2025-07-29\n",
      "\n",
      "üî¨ Testing download of most recent 10-K or 8-K filing...\n",
      "üì• Downloading 8-K from 2025-07-31...\n",
      "‚ö†Ô∏è Could not download filing. Status: 404\n",
      "\n",
      "============================================================\n",
      "üìã SEC filing download test complete!\n",
      "üéØ Next: We'll test news API connections...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# We'll test with Apple Inc. (everyone knows them, lots of filings)\n",
    "test_company = \"Apple Inc\"\n",
    "test_ticker = \"AAPL\" \n",
    "apple_cik = \"0000320193\"  # Apple's official SEC identifier\n",
    "\n",
    "# SEC API endpoint for company filings\n",
    "filings_url = f\"https://data.sec.gov/submissions/CIK{apple_cik}.json\"\n",
    "\n",
    "# Set up headers (SEC requirement)\n",
    "headers = {\n",
    "    'User-Agent': 'M&A Intelligence Platform (dhruv.student@example.com)',  # Update with your email\n",
    "    'Accept-Encoding': 'gzip, deflate',\n",
    "    'Host': 'data.sec.gov'\n",
    "}\n",
    "\n",
    "try:\n",
    "    print(f\"üîç Looking up recent filings for {test_company} ({test_ticker})...\")\n",
    "    \n",
    "    # Get company's filing information\n",
    "    response = requests.get(filings_url, headers=headers, timeout=15)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        print(\"‚úÖ Successfully downloaded company data!\")\n",
    "        \n",
    "        # Parse the JSON response\n",
    "        company_info = response.json()\n",
    "        \n",
    "        # Extract basic company information\n",
    "        company_name = company_info.get('name', 'Unknown')\n",
    "        sic_description = company_info.get('sicDescription', 'Unknown')\n",
    "        \n",
    "        print(f\"üè¢ Company: {company_name}\")\n",
    "        print(f\"üìä Industry: {sic_description}\")\n",
    "        \n",
    "        # Get recent filings\n",
    "        recent_filings = company_info.get('filings', {}).get('recent', {})\n",
    "        \n",
    "        if recent_filings:\n",
    "            filing_forms = recent_filings.get('form', [])\n",
    "            filing_dates = recent_filings.get('filingDate', [])\n",
    "            accession_numbers = recent_filings.get('accessionNumber', [])\n",
    "            \n",
    "            print(f\"\\nüìã Found {len(filing_forms)} recent filings\")\n",
    "            \n",
    "            # Show the 5 most recent filings\n",
    "            print(\"\\nüóÇÔ∏è Most Recent Filings:\")\n",
    "            for i in range(min(5, len(filing_forms))):\n",
    "                form_type = filing_forms[i]\n",
    "                filing_date = filing_dates[i]\n",
    "                \n",
    "                # Highlight M&A-relevant filing types\n",
    "                if form_type in ['10-K', '10-Q', '8-K', 'DEF 14A']:\n",
    "                    marker = \"üéØ\"  # These often contain M&A signals\n",
    "                else:\n",
    "                    marker = \"üìÑ\"\n",
    "                    \n",
    "                print(f\"   {marker} {form_type} filed on {filing_date}\")\n",
    "            \n",
    "            # Test downloading one actual filing\n",
    "            print(f\"\\nüî¨ Testing download of most recent 10-K or 8-K filing...\")\n",
    "            \n",
    "            # Find a 10-K or 8-K filing (most likely to have M&A content)\n",
    "            target_filing = None\n",
    "            for i in range(len(filing_forms)):\n",
    "                if filing_forms[i] in ['10-K', '8-K']:\n",
    "                    target_filing = {\n",
    "                        'form': filing_forms[i],\n",
    "                        'date': filing_dates[i],\n",
    "                        'accession': accession_numbers[i].replace('-', '')\n",
    "                    }\n",
    "                    break\n",
    "            \n",
    "            if target_filing:\n",
    "                # Construct URL for the actual filing document\n",
    "                accession_clean = target_filing['accession']\n",
    "                accession_formatted = f\"{accession_clean[:10]}-{accession_clean[10:12]}-{accession_clean[12:]}\"\n",
    "                \n",
    "                filing_url = f\"https://www.sec.gov/Archives/edgar/data/{apple_cik}/{accession_clean}/{accession_formatted}.txt\"\n",
    "                \n",
    "                print(f\"üì• Downloading {target_filing['form']} from {target_filing['date']}...\")\n",
    "                \n",
    "                # Add a small delay to be respectful to SEC servers\n",
    "                time.sleep(0.1)\n",
    "                \n",
    "                filing_response = requests.get(filing_url, headers=headers, timeout=15)\n",
    "                \n",
    "                if filing_response.status_code == 200:\n",
    "                    filing_text = filing_response.text\n",
    "                    word_count = len(filing_text.split())\n",
    "                    \n",
    "                    print(f\"‚úÖ SUCCESS: Downloaded {target_filing['form']} filing!\")\n",
    "                    print(f\"üìä Document length: {word_count:,} words\")\n",
    "                    \n",
    "                    # Quick test: look for M&A-related keywords\n",
    "                    ma_keywords = ['acquisition', 'merger', 'strategic', 'divest', 'spin-off', 'restructur']\n",
    "                    keyword_counts = {}\n",
    "                    \n",
    "                    for keyword in ma_keywords:\n",
    "                        count = filing_text.lower().count(keyword)\n",
    "                        if count > 0:\n",
    "                            keyword_counts[keyword] = count\n",
    "                    \n",
    "                    if keyword_counts:\n",
    "                        print(f\"\\nüéØ M&A-related keywords found:\")\n",
    "                        for word, count in keyword_counts.items():\n",
    "                            print(f\"   ‚Ä¢ '{word}': {count} mentions\")\n",
    "                    else:\n",
    "                        print(f\"\\nüìù No major M&A keywords in this filing (normal for {target_filing['form']})\")\n",
    "                    \n",
    "                    print(f\"\\nüöÄ Ready to process SEC filings! System is working perfectly.\")\n",
    "                    \n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è Could not download filing. Status: {filing_response.status_code}\")\n",
    "                    \n",
    "            else:\n",
    "                print(\"üìã No 10-K or 8-K filings found in recent submissions\")\n",
    "                \n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No recent filings data available\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"‚ùå Failed to get company data. Status code: {response.status_code}\")\n",
    "        print(\"SEC might be busy - try again in a few minutes\")\n",
    "        \n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"‚ùå Network error: {str(e)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d01da694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Installing feedparser for RSS feeds...\n",
      "üîç Testing RSS news feeds...\n",
      "\n",
      "üì° Testing Reuters Business...\n",
      "‚ö†Ô∏è No articles found in Reuters Business feed\n",
      "\n",
      "üì° Testing MarketWatch...\n",
      "‚úÖ Success! Found 10 recent articles\n",
      "üéØ Found 1 M&A-related articles:\n",
      "   ‚Ä¢ EchoStar‚Äôs stock is surging. Why AT&T just struck a $23 billion spectrum deal wi...\n",
      "\n",
      "üì° Testing Yahoo Finance...\n",
      "‚úÖ Success! Found 45 recent articles\n",
      "üéØ Found 1 M&A-related articles:\n",
      "   ‚Ä¢ MARA Holdings Signs Investment Agreement with EDF Plus Ventures to Acquire Exaio...\n",
      "\n",
      "üì° Testing SEC Press Releases...\n",
      "‚úÖ Success! Found 25 recent articles\n",
      "üéØ Found 1 M&A-related articles:\n",
      "   ‚Ä¢ Staff Issues FAQs to Help Broker-Dealers Implement Financial Responsibility Requ...\n",
      "\n",
      "============================================================\n",
      "üìä NEWS SOURCES SUMMARY:\n",
      "‚úÖ Working sources: 3/4\n",
      "üéØ Total M&A articles found: 3\n",
      "\n",
      "üöÄ Active news sources:\n",
      "   ‚Ä¢ MarketWatch\n",
      "   ‚Ä¢ Yahoo Finance\n",
      "   ‚Ä¢ SEC Press Releases\n",
      "\n",
      "üì∞ SAMPLE M&A ARTICLE:\n",
      "Title: EchoStar‚Äôs stock is surging. Why AT&T just struck a $23 billion spectrum deal with the company.\n",
      "Source: MarketWatch\n",
      "Date: Tue, 26 Aug 2025 12:12:00 GMT\n",
      "M&A Keyword: 'deal'\n",
      "\n",
      "üéØ News collection system ready!\n",
      "üìã Next: We'll test financial data APIs...\n"
     ]
    }
   ],
   "source": [
    "# Getting in the RSS news feeds\n",
    "\n",
    "# Install feedparser if not already installed\n",
    "try:\n",
    "    import feedparser\n",
    "except ImportError:\n",
    "    print(\"üì¶ Installing feedparser for RSS feeds...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"feedparser\"])\n",
    "    import feedparser\n",
    "\n",
    "# Test multiple free news sources\n",
    "news_sources = {\n",
    "    \"Reuters Business\": \"http://feeds.reuters.com/reuters/businessNews\",\n",
    "    \"MarketWatch\": \"http://feeds.marketwatch.com/marketwatch/topstories/\", \n",
    "    \"Yahoo Finance\": \"https://finance.yahoo.com/news/rssindex\",\n",
    "    \"SEC Press Releases\": \"https://www.sec.gov/news/pressreleases.rss\"\n",
    "}\n",
    "\n",
    "print(\"üîç Testing RSS news feeds...\")\n",
    "\n",
    "successful_sources = []\n",
    "all_articles = []\n",
    "\n",
    "for source_name, rss_url in news_sources.items():\n",
    "    try:\n",
    "        print(f\"\\nüì° Testing {source_name}...\")\n",
    "        \n",
    "        # Parse RSS feed\n",
    "        feed = feedparser.parse(rss_url)\n",
    "        \n",
    "        if feed.entries:\n",
    "            article_count = len(feed.entries)\n",
    "            print(f\"‚úÖ Success! Found {article_count} recent articles\")\n",
    "            \n",
    "            # Look for M&A related articles\n",
    "            ma_articles = []\n",
    "            ma_keywords = ['merger', 'acquisition', 'buyout', 'takeover', 'deal', 'acquire', 'divest']\n",
    "            \n",
    "            for entry in feed.entries[:10]:  # Check first 10 articles\n",
    "                title = entry.get('title', '').lower()\n",
    "                summary = entry.get('summary', '').lower()\n",
    "                \n",
    "                # Check if article contains M&A keywords\n",
    "                for keyword in ma_keywords:\n",
    "                    if keyword in title or keyword in summary:\n",
    "                        ma_articles.append({\n",
    "                            'title': entry.get('title', 'No title'),\n",
    "                            'published': entry.get('published', 'No date'),\n",
    "                            'link': entry.get('link', ''),\n",
    "                            'source': source_name,\n",
    "                            'keyword': keyword\n",
    "                        })\n",
    "                        break\n",
    "            \n",
    "            if ma_articles:\n",
    "                print(f\"üéØ Found {len(ma_articles)} M&A-related articles:\")\n",
    "                for article in ma_articles[:3]:  # Show first 3\n",
    "                    print(f\"   ‚Ä¢ {article['title'][:80]}...\")\n",
    "                    \n",
    "                all_articles.extend(ma_articles)\n",
    "            else:\n",
    "                print(\"üìã No M&A articles in recent headlines (normal - deals are rare)\")\n",
    "                \n",
    "            successful_sources.append(source_name)\n",
    "            \n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è No articles found in {source_name} feed\")\n",
    "            \n",
    "        # Small delay to be respectful\n",
    "        time.sleep(0.2)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error accessing {source_name}: {str(e)}\")\n",
    "\n",
    "# Summary of results\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä NEWS SOURCES SUMMARY:\")\n",
    "print(f\"‚úÖ Working sources: {len(successful_sources)}/{len(news_sources)}\")\n",
    "print(f\"üéØ Total M&A articles found: {len(all_articles)}\")\n",
    "\n",
    "if successful_sources:\n",
    "    print(f\"\\nüöÄ Active news sources:\")\n",
    "    for source in successful_sources:\n",
    "        print(f\"   ‚Ä¢ {source}\")\n",
    "\n",
    "# Test web scraping backup (if RSS fails)\n",
    "if len(successful_sources) < 2:\n",
    "    print(f\"\\nüîß Testing backup: Web scraping MarketWatch M&A section...\")\n",
    "    \n",
    "    try:\n",
    "        # Test scraping MarketWatch M&A page\n",
    "        marketwatch_url = \"https://www.marketwatch.com/markets\"\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(marketwatch_url, headers=headers, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(\"‚úÖ Web scraping backup working!\")\n",
    "            print(\"üí° Can scrape financial news sites directly if RSS feeds fail\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Web scraping test failed: Status {response.status_code}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Web scraping test error: {str(e)}\")\n",
    "\n",
    "# Show sample M&A article if found\n",
    "if all_articles:\n",
    "    print(f\"\\nüì∞ SAMPLE M&A ARTICLE:\")\n",
    "    sample = all_articles[0]\n",
    "    print(f\"Title: {sample['title']}\")\n",
    "    print(f\"Source: {sample['source']}\")  \n",
    "    print(f\"Date: {sample['published']}\")\n",
    "    print(f\"M&A Keyword: '{sample['keyword']}'\")\n",
    "\n",
    "print(f\"\\nüéØ News collection system ready!\")\n",
    "print(\"üìã Next: We'll test financial data APIs...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
