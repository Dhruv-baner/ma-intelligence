{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75bf101c",
   "metadata": {},
   "source": [
    "# 📋 Notebook 2: News Intelligence - Complete Overview\n",
    "\n",
    "## 🎯 What This Notebook Is For\n",
    "\n",
    "Think of this notebook as **building an intelligent newspaper reader** that works 24/7. While Notebook 1 set up our kitchen, this notebook creates a smart assistant that reads hundreds of business articles every day and tells us which ones are about mergers and acquisitions.\n",
    "\n",
    "**In simple terms:** We're creating a system that automatically collects business news, finds articles about companies buying or selling each other, analyzes whether the news is positive or negative, and creates daily briefings that summarize all the M&A activity happening in the market.\n",
    "\n",
    "**Real-world value:** Investment bankers pay teams of analysts to read news all day looking for M&A opportunities. Our AI system does this automatically and never misses a story.\n",
    "\n",
    "---\n",
    "\n",
    "## 🏗️ Why We Need News Intelligence\n",
    "\n",
    "Imagine you're trying to stay updated on everything happening in your neighborhood. You could:\n",
    "- **Read every local newspaper** (time-consuming and you might miss some)\n",
    "- **Ask friends to tell you news** (unreliable and incomplete)\n",
    "- **Set up Google alerts** (helpful but still requires manual reading)\n",
    "- **Build an AI assistant** that reads everything and summarizes only what matters ✅\n",
    "\n",
    "Similarly, for M&A intelligence, there are thousands of business articles published daily across hundreds of news sources. Our AI system will:\n",
    "- **Automatically collect** articles from major business news sources\n",
    "- **Filter for relevance** - only flag articles containing M&A keywords\n",
    "- **Analyze sentiment** - determine if the news is positive, negative, or neutral\n",
    "- **Link to companies** - connect news stories to companies in our database\n",
    "- **Generate daily briefings** - create executive summaries of all M&A activity\n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 Technical Foundation (Simplified)\n",
    "\n",
    "We're building four main components:\n",
    "\n",
    "### 📰 **Automated News Collection**\n",
    "- **What it is:** Like having a robot that visits every major business news website daily and downloads new articles\n",
    "- **Why we need it:** M&A deals are first announced in business news, so we need to catch them immediately\n",
    "- **How it works:** RSS feeds and web scraping to automatically download articles from Reuters, MarketWatch, Yahoo Finance, etc.\n",
    "\n",
    "### 🧠 **AI Text Analysis**\n",
    "- **What it is:** Teaching our computer to \"read\" and understand news articles like a human would\n",
    "- **Why we need it:** We need to automatically identify which articles are about M&A and determine if they're positive or negative news\n",
    "- **How it works:** Natural Language Processing (NLP) to detect M&A keywords and sentiment analysis\n",
    "\n",
    "### 🗄️ **News Database System**\n",
    "- **What it is:** A organized storage system for all the articles we collect, linked to our company database\n",
    "- **Why we need it:** We need to store, search, and analyze thousands of articles over time\n",
    "- **How it works:** SQLite tables that link news articles to specific companies and track sentiment over time\n",
    "\n",
    "### 📋 **Daily Briefing Generator**\n",
    "- **What it is:** An AI system that reads all the day's M&A news and writes executive-style summaries\n",
    "- **Why we need it:** Busy executives want summaries, not hundreds of individual articles\n",
    "- **How it works:** Automated report generation that ranks stories by importance and creates readable summaries\n",
    "\n",
    "---\n",
    "\n",
    "## 📋 Step-by-Step Breakdown\n",
    "\n",
    "### **Cell 1: Setup & Libraries** 📚\n",
    "**What we're doing:** Loading all the AI and web scraping tools we need\n",
    "**Simple analogy:** Getting your reading glasses, notebooks, and highlighters before reading the newspaper\n",
    "**Key tools:** RSS readers, web scrapers, sentiment analyzers, database connectors\n",
    "\n",
    "### **Cell 2: News Database Creation** 🗄️\n",
    "**What we're doing:** Creating database tables to store news articles and link them to companies\n",
    "**Simple analogy:** Setting up a filing system with folders for each company and each type of news\n",
    "**Database structure:** Tables for articles, sentiment scores, company links, and daily summaries\n",
    "\n",
    "### **Cell 3: RSS Feed Collection** 📡\n",
    "**What we're doing:** Automatically downloading articles from major business news RSS feeds\n",
    "**Simple analogy:** Like subscribing to multiple newspapers and having them delivered daily\n",
    "**News sources:** Reuters, MarketWatch, Yahoo Finance, SEC press releases\n",
    "**Output:** Raw article data with headlines, publication dates, and content\n",
    "\n",
    "### **Cell 4: M&A Article Filtering** 🔍\n",
    "**What we're doing:** Using AI to identify which articles are actually about mergers and acquisitions\n",
    "**Simple analogy:** Like having an assistant read through all newspapers and only show you articles about house sales\n",
    "**M&A keywords:** \"merger\", \"acquisition\", \"buyout\", \"takeover\", \"strategic review\", \"divest\"\n",
    "**Output:** Filtered list of only M&A-relevant articles\n",
    "\n",
    "### **Cell 5: Sentiment Analysis** 💭\n",
    "**What we're doing:** Using AI to determine if each M&A article contains positive, negative, or neutral news\n",
    "**Simple analogy:** Like having someone read each article and tell you if it's good news or bad news\n",
    "**AI technique:** VADER sentiment analysis specifically designed for news and social media\n",
    "**Output:** Sentiment scores (-1 to +1) for each article\n",
    "\n",
    "### **Cell 6: Company Linking** 🔗\n",
    "**What we're doing:** Connecting each news article to specific companies in our database\n",
    "**Simple analogy:** Like sorting newspaper clippings into folders for each person/company mentioned\n",
    "**Matching process:** Search article text for company names and stock tickers from our database\n",
    "**Output:** Articles tagged with relevant company IDs\n",
    "\n",
    "### **Cell 7: Daily Briefing Generation** 📋\n",
    "**What we're doing:** Creating automated daily summaries of all M&A news\n",
    "**Simple analogy:** Like having a personal assistant read all the news and give you a 5-minute briefing\n",
    "**Report contents:** Top stories, market trends, company highlights, sentiment analysis\n",
    "**Output:** Professional executive briefing ready for email or dashboard\n",
    "\n",
    "### **Cell 8: Historical Analysis** 📈\n",
    "**What we're doing:** Analyzing patterns in news coverage to identify trends and cycles\n",
    "**Simple analogy:** Like looking at months of weather reports to predict seasonal patterns\n",
    "**Analysis types:** Volume trends, sentiment patterns, sector activity, deal timing\n",
    "**Output:** Insights about M&A market cycles and news patterns\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Planned Cell Summary Table\n",
    "\n",
    "| Step | Purpose | Key Technology | Expected Output |\n",
    "|------|---------|----------------|----------------|\n",
    "| **Cell 1** | Setup AI Tools | NLP Libraries, Database Connection | All tools ready for news analysis |\n",
    "| **Cell 2** | Database Structure | SQLite Tables | News storage system ready |\n",
    "| **Cell 3** | Collect Articles | RSS Feed Parsing | 50-100 raw business articles |\n",
    "| **Cell 4** | Filter M&A News | Keyword Matching | 5-15 M&A-relevant articles |\n",
    "| **Cell 5** | Analyze Sentiment | VADER Sentiment Analysis | Positive/negative scores for each article |\n",
    "| **Cell 6** | Link Companies | Text Matching | Articles connected to specific companies |\n",
    "| **Cell 7** | Daily Briefing | Automated Report Generation | Executive summary of daily M&A activity |\n",
    "| **Cell 8** | Historical Patterns | Trend Analysis | Insights about M&A news cycles |\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 What We Will Accomplish\n",
    "\n",
    "**By the end of this notebook, we'll have built a complete news intelligence system:**\n",
    "\n",
    "🎯 **Automated daily news collection** - System that runs every day to gather M&A articles\n",
    "🎯 **AI-powered article analysis** - Computer that \"reads\" and understands business news  \n",
    "🎯 **Professional database storage** - Organized system for storing and searching thousands of articles\n",
    "🎯 **Company-specific news tracking** - Ability to see all news about any company over time\n",
    "🎯 **Daily executive briefings** - Automated summaries ready for business professionals\n",
    "🎯 **Sentiment tracking** - Understanding whether M&A news is positive or negative for companies\n",
    "🎯 **Market trend analysis** - Insights into M&A activity patterns and cycles\n",
    "\n",
    "---\n",
    "\n",
    "## 🔄 How This Connects to Our Overall M&A System\n",
    "\n",
    "**Notebook 1** built the data foundation - our ability to collect information about companies.\n",
    "\n",
    "**Notebook 2** builds the news intelligence layer - our ability to understand what's happening in the market right now.\n",
    "\n",
    "**Future notebooks** will combine this real-time news intelligence with our company analysis to predict which companies are likely to be involved in future M&A deals.\n",
    "\n",
    "**Think of it like this:**\n",
    "- **Notebook 1:** Built our research library (company data)\n",
    "- **Notebook 2:** Hired a smart newspaper reader (news intelligence) ← We are here\n",
    "- **Notebook 3:** Will hire document analysts (SEC filing analysis)\n",
    "- **Notebook 4:** Will build the prediction engine (AI models that combine everything)\n",
    "\n",
    "---\n",
    "\n",
    "## 💼 Business Value\n",
    "\n",
    "**This news intelligence system alone is valuable because:**\n",
    "\n",
    "✅ **Investment banks** pay analysts $100K+ salaries just to read and summarize M&A news daily\n",
    "✅ **Private equity firms** need to stay updated on all market activity to spot opportunities  \n",
    "✅ **Corporate development teams** must track competitor M&A activity and market trends\n",
    "✅ **Consultants** bill clients for market intelligence and trend analysis\n",
    "\n",
    "**Our automated system does all of this 24/7 without human intervention.**\n",
    "\n",
    "---\n",
    "\n",
    "## ➡️ Success Metrics for This Notebook\n",
    "\n",
    "**We'll know this notebook succeeded when:**\n",
    "- ✅ We can automatically collect 50+ business articles per day\n",
    "- ✅ AI correctly identifies 80%+ of M&A-relevant articles  \n",
    "- ✅ Sentiment analysis provides meaningful positive/negative scores\n",
    "- ✅ Articles are properly linked to companies in our database\n",
    "- ✅ Daily briefings read like professional executive summaries\n",
    "- ✅ System runs reliably without manual intervention\n",
    "\n",
    "---\n",
    "\n",
    "*This notebook transforms us from having company data to having real-time market intelligence. Combined with our prediction models, this will give us the early warning system that investment professionals pay millions to access.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04a194a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📰 Setting up M&A News Intelligence System\n",
      "============================================================\n",
      "✅ NLP libraries loaded\n",
      "✅ NLTK data already available\n",
      "✅ Configuration loaded from Notebook 1\n",
      "✅ Connected to database: ../data/processed/ma_intelligence.db\n",
      "\n",
      "📊 NEWS INTELLIGENCE SETUP COMPLETE!\n",
      "🎯 M&A Keywords: ['merger', 'acquisition', 'buyout', 'takeover', 'deal', 'acquire', 'divest', 'strategic review', 'strategic alternatives', 'spin-off', 'restructuring', 'consolidation']\n",
      "📡 News Sources: 4 RSS feeds configured\n",
      "🗄️ Database: Ready for article storage and analysis\n",
      "📅 Session started: 2025-08-27 15:33:52\n",
      "\n",
      "🚀 Ready to collect and analyze M&A news!\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup News Intelligence System\n",
    "print(\"📰 Setting up M&A News Intelligence System\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Core libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import sqlite3\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "# RSS feed processing\n",
    "import feedparser\n",
    "\n",
    "# Web scraping\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Text analysis and NLP\n",
    "try:\n",
    "    import nltk\n",
    "    from textblob import TextBlob\n",
    "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "    print(\"✅ NLP libraries loaded\")\n",
    "except ImportError as e:\n",
    "    print(f\"📦 Installing missing NLP libraries: {e}\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    \n",
    "    # Install required packages\n",
    "    packages = ['nltk', 'textblob', 'vaderSentiment']\n",
    "    for package in packages:\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        except:\n",
    "            print(f\"⚠️ Could not install {package}\")\n",
    "    \n",
    "    # Try importing again\n",
    "    import nltk\n",
    "    from textblob import TextBlob\n",
    "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "    print(\"✅ NLP libraries installed and loaded\")\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    print(\"✅ NLTK data already available\")\n",
    "except LookupError:\n",
    "    print(\"📥 Downloading NLTK data...\")\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('vader_lexicon', quiet=True)\n",
    "    print(\"✅ NLTK data downloaded\")\n",
    "\n",
    "# Configuration and database\n",
    "sys.path.append('../src')\n",
    "try:\n",
    "    from config_loader import load_config, load_data_sources, get_database_path\n",
    "    config = load_config()\n",
    "    data_sources = load_data_sources()\n",
    "    print(\"✅ Configuration loaded from Notebook 1\")\n",
    "except ImportError:\n",
    "    print(\"⚠️ Could not load configuration from Notebook 1\")\n",
    "    print(\"💡 Will use backup configuration\")\n",
    "    \n",
    "    # Backup configuration\n",
    "    config = {\n",
    "        'news_intelligence': {\n",
    "            'ma_keywords': ['merger', 'acquisition', 'buyout', 'takeover', 'deal', 'acquire', 'divest'],\n",
    "            'max_articles_per_source': 50\n",
    "        }\n",
    "    }\n",
    "    data_sources = {\n",
    "        'news_sources': {\n",
    "            'rss_feeds': [\n",
    "                {'name': 'Reuters Business', 'url': 'http://feeds.reuters.com/reuters/businessNews', 'priority': 'high'},\n",
    "                {'name': 'MarketWatch', 'url': 'http://feeds.marketwatch.com/marketwatch/topstories/', 'priority': 'high'},\n",
    "                {'name': 'Yahoo Finance', 'url': 'https://finance.yahoo.com/news/rssindex', 'priority': 'medium'}\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Initialize sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Database connection\n",
    "try:\n",
    "    db_path = get_database_path() if 'get_database_path' in globals() else \"../data/processed/ma_intelligence.db\"\n",
    "    db_connection = sqlite3.connect(db_path)\n",
    "    print(f\"✅ Connected to database: {db_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Database connection issue: {e}\")\n",
    "    db_path = \"../data/processed/ma_intelligence.db\"\n",
    "    db_connection = sqlite3.connect(db_path)\n",
    "    print(f\"✅ Connected to backup database path\")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "print(f\"\\n📊 NEWS INTELLIGENCE SETUP COMPLETE!\")\n",
    "print(f\"🎯 M&A Keywords: {config['news_intelligence']['ma_keywords']}\")\n",
    "print(f\"📡 News Sources: {len(data_sources['news_sources']['rss_feeds'])} RSS feeds configured\")\n",
    "print(f\"🗄️ Database: Ready for article storage and analysis\")\n",
    "print(f\"📅 Session started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "print(f\"\\n🚀 Ready to collect and analyze M&A news!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8cc6d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏗️ Creating news intelligence tables...\n",
      "✅ All tables created successfully!\n",
      "⚡ Creating database indexes for fast queries...\n",
      "✅ Database indexes created!\n",
      "📡 Setting up news sources...\n",
      "\n",
      "📊 DATABASE STRUCTURE SUMMARY:\n",
      "🏢 Companies in system: 40\n",
      "📡 News sources configured: 6\n",
      "📰 Articles stored: 0 (will increase as we collect)\n",
      "🤝 M&A deals tracked: 0 (will populate with 2025 data)\n",
      "\n",
      "🗄️ MAIN TABLES CREATED:\n",
      "   📋 news_articles: 18 columns\n",
      "   📋 article_companies: 8 columns\n",
      "   📋 ma_deals_2025: 22 columns\n",
      "   📋 daily_summaries: 13 columns\n",
      "   📋 news_sources: 12 columns\n",
      "\n",
      "💡 KEY QUERY EXAMPLES:\n",
      "   • Find today's M&A articles:\n",
      "     SELECT * FROM news_articles WHERE published_date >= date('now') AND ma_relevance_score > 0.7\n",
      "   • Get all news for a specific company:\n",
      "     SELECT a.* FROM news_articles a JOIN article_companies ac ON a.article_id = ac.article_id WHERE ac.company_ticker = 'AAPL'\n",
      "   • Track sentiment trends:\n",
      "     SELECT DATE(published_date), AVG(sentiment_score) FROM news_articles GROUP BY DATE(published_date)\n",
      "   • Monitor deal pipeline:\n",
      "     SELECT * FROM ma_deals_2025 WHERE deal_status = 'announced' ORDER BY announcement_date DESC\n",
      "\n",
      "🔬 Testing database operations...\n",
      "✅ Database read/write operations working correctly!\n",
      "\n",
      "============================================================\n",
      "🗄️ NEWS INTELLIGENCE DATABASE READY!\n",
      "📊 Designed to handle:\n",
      "   • Live daily news collection (unlimited articles)\n",
      "   • Historical 2025 M&A validation data\n",
      "   • Company-article relationships\n",
      "   • Sentiment analysis results\n",
      "   • Deal tracking and validation\n",
      "   • Automated daily briefing generation\n",
      "\n",
      "🚀 Ready for Cell 3: News Collection System!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhruv\\AppData\\Local\\Temp\\ipykernel_32228\\603973310.py:254: DeprecationWarning: The default date adapter is deprecated as of Python 3.12; see the sqlite3 documentation for suggested replacement recipes\n",
      "  cursor.execute('''\n",
      "C:\\Users\\dhruv\\AppData\\Local\\Temp\\ipykernel_32228\\603973310.py:260: DeprecationWarning: The default date adapter is deprecated as of Python 3.12; see the sqlite3 documentation for suggested replacement recipes\n",
      "  cursor.execute('SELECT * FROM daily_summaries WHERE summary_date = ?', (datetime.now().date(),))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Connect to our main database\n",
    "cursor = db_connection.cursor()\n",
    "\n",
    "print(\"🏗️ Creating news intelligence tables...\")\n",
    "\n",
    "# 1. Main articles table - stores all news articles (live + historical)\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS news_articles (\n",
    "    article_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    \n",
    "    -- Article content\n",
    "    headline TEXT NOT NULL,\n",
    "    summary TEXT,\n",
    "    full_text TEXT,\n",
    "    url TEXT UNIQUE,\n",
    "    \n",
    "    -- Source information\n",
    "    source_name VARCHAR(100) NOT NULL,\n",
    "    author VARCHAR(200),\n",
    "    published_date DATETIME NOT NULL,\n",
    "    \n",
    "    -- Article classification\n",
    "    article_type VARCHAR(20) DEFAULT 'live',  -- 'live', 'historical', 'archive'\n",
    "    ma_relevance_score REAL DEFAULT 0.0,     -- 0-1: how M&A-relevant is this article\n",
    "    ma_keywords_found TEXT,                   -- JSON list of M&A keywords detected\n",
    "    \n",
    "    -- Sentiment analysis\n",
    "    sentiment_score REAL,                     -- -1 (negative) to +1 (positive)\n",
    "    sentiment_label VARCHAR(20),              -- 'positive', 'negative', 'neutral'\n",
    "    confidence_score REAL,                    -- How confident we are in the sentiment\n",
    "    \n",
    "    -- Processing metadata\n",
    "    processed_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n",
    "    processing_version VARCHAR(10) DEFAULT '1.0',\n",
    "    \n",
    "    -- Content analysis\n",
    "    word_count INTEGER,\n",
    "    language VARCHAR(10) DEFAULT 'en',\n",
    "    \n",
    "    UNIQUE(url, published_date)\n",
    ")\n",
    "''')\n",
    "\n",
    "# 2. Company mentions table - links articles to specific companies\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS article_companies (\n",
    "    mention_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    article_id INTEGER NOT NULL,\n",
    "    company_ticker VARCHAR(10) NOT NULL,\n",
    "    \n",
    "    -- How the company was mentioned\n",
    "    mention_type VARCHAR(20),                 -- 'acquirer', 'target', 'mentioned', 'competitor'\n",
    "    mention_context TEXT,                     -- Sentence where company was mentioned\n",
    "    confidence_score REAL DEFAULT 1.0,       -- How sure we are about this link\n",
    "    \n",
    "    -- Company role in M&A context\n",
    "    ma_role VARCHAR(20),                      -- 'buyer', 'seller', 'advisor', 'related'\n",
    "    \n",
    "    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n",
    "    \n",
    "    FOREIGN KEY (article_id) REFERENCES news_articles(article_id),\n",
    "    FOREIGN KEY (company_ticker) REFERENCES companies(ticker),\n",
    "    UNIQUE(article_id, company_ticker)\n",
    ")\n",
    "''')\n",
    "\n",
    "# 3. M&A deals table - track actual deals for validation and historical context\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS ma_deals_2025 (\n",
    "    deal_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    \n",
    "    -- Deal basics\n",
    "    deal_name VARCHAR(200) NOT NULL,\n",
    "    announcement_date DATE NOT NULL,\n",
    "    expected_completion_date DATE,\n",
    "    actual_completion_date DATE,\n",
    "    \n",
    "    -- Companies involved\n",
    "    acquirer_ticker VARCHAR(10),\n",
    "    acquirer_name VARCHAR(200) NOT NULL,\n",
    "    target_ticker VARCHAR(10),\n",
    "    target_name VARCHAR(200) NOT NULL,\n",
    "    \n",
    "    -- Deal details\n",
    "    deal_value_billions REAL,                -- Deal value in billions USD\n",
    "    deal_type VARCHAR(30),                   -- 'merger', 'acquisition', 'spinoff', 'joint_venture'\n",
    "    deal_status VARCHAR(20) DEFAULT 'announced', -- 'announced', 'pending', 'completed', 'failed', 'withdrawn'\n",
    "    \n",
    "    -- Business context\n",
    "    primary_sector VARCHAR(100),\n",
    "    deal_rationale TEXT,                     -- Strategic reasoning for the deal\n",
    "    synergies_expected_millions REAL,        -- Expected cost synergies\n",
    "    \n",
    "    -- Market impact\n",
    "    premium_percent REAL,                    -- Premium paid over market price\n",
    "    financing_method VARCHAR(50),            -- 'cash', 'stock', 'mixed'\n",
    "    \n",
    "    -- Validation tracking\n",
    "    predicted_by_system BOOLEAN DEFAULT 0,  -- Did our system predict this?\n",
    "    prediction_date DATE,                    -- When did we predict it?\n",
    "    prediction_confidence REAL,             -- What was our confidence level?\n",
    "    \n",
    "    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n",
    "    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    ")\n",
    "''')\n",
    "\n",
    "# 4. Daily news summaries table - store generated briefings\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS daily_summaries (\n",
    "    summary_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    summary_date DATE NOT NULL UNIQUE,\n",
    "    \n",
    "    -- Content\n",
    "    executive_summary TEXT,                  -- High-level summary for executives\n",
    "    key_stories TEXT,                       -- JSON array of top stories\n",
    "    market_sentiment VARCHAR(20),           -- Overall market sentiment that day\n",
    "    \n",
    "    -- Statistics\n",
    "    total_articles_collected INTEGER DEFAULT 0,\n",
    "    ma_articles_identified INTEGER DEFAULT 0,\n",
    "    deals_announced INTEGER DEFAULT 0,\n",
    "    deals_completed INTEGER DEFAULT 0,\n",
    "    \n",
    "    -- Sector analysis\n",
    "    most_active_sector VARCHAR(100),\n",
    "    sector_breakdown TEXT,                   -- JSON with sector activity counts\n",
    "    \n",
    "    -- Generated content\n",
    "    generated_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n",
    "    generation_version VARCHAR(10) DEFAULT '1.0'\n",
    ")\n",
    "''')\n",
    "\n",
    "# 5. News sources tracking table - monitor source reliability\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS news_sources (\n",
    "    source_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    source_name VARCHAR(100) NOT NULL UNIQUE,\n",
    "    source_url TEXT,\n",
    "    source_type VARCHAR(20),                 -- 'rss', 'api', 'scraping'\n",
    "    \n",
    "    -- Reliability metrics\n",
    "    total_articles_collected INTEGER DEFAULT 0,\n",
    "    ma_articles_found INTEGER DEFAULT 0,\n",
    "    accuracy_score REAL DEFAULT 0.0,        -- How often their M&A articles are accurate\n",
    "    \n",
    "    -- Operational status\n",
    "    last_successful_collection DATETIME,\n",
    "    last_failed_collection DATETIME,\n",
    "    consecutive_failures INTEGER DEFAULT 0,\n",
    "    status VARCHAR(20) DEFAULT 'active',     -- 'active', 'inactive', 'error'\n",
    "    \n",
    "    created_at DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    ")\n",
    "''')\n",
    "\n",
    "print(\"✅ All tables created successfully!\")\n",
    "\n",
    "# Create indexes for better performance\n",
    "print(\"⚡ Creating database indexes for fast queries...\")\n",
    "\n",
    "# Articles table indexes\n",
    "cursor.execute('CREATE INDEX IF NOT EXISTS idx_articles_date ON news_articles(published_date)')\n",
    "cursor.execute('CREATE INDEX IF NOT EXISTS idx_articles_source ON news_articles(source_name)')\n",
    "cursor.execute('CREATE INDEX IF NOT EXISTS idx_articles_ma_relevance ON news_articles(ma_relevance_score)')\n",
    "cursor.execute('CREATE INDEX IF NOT EXISTS idx_articles_sentiment ON news_articles(sentiment_score)')\n",
    "cursor.execute('CREATE INDEX IF NOT EXISTS idx_articles_type ON news_articles(article_type)')\n",
    "\n",
    "# Company mentions indexes\n",
    "cursor.execute('CREATE INDEX IF NOT EXISTS idx_mentions_article ON article_companies(article_id)')\n",
    "cursor.execute('CREATE INDEX IF NOT EXISTS idx_mentions_company ON article_companies(company_ticker)')\n",
    "cursor.execute('CREATE INDEX IF NOT EXISTS idx_mentions_role ON article_companies(ma_role)')\n",
    "\n",
    "# Deals indexes  \n",
    "cursor.execute('CREATE INDEX IF NOT EXISTS idx_deals_date ON ma_deals_2025(announcement_date)')\n",
    "cursor.execute('CREATE INDEX IF NOT EXISTS idx_deals_acquirer ON ma_deals_2025(acquirer_ticker)')\n",
    "cursor.execute('CREATE INDEX IF NOT EXISTS idx_deals_target ON ma_deals_2025(target_ticker)')\n",
    "cursor.execute('CREATE INDEX IF NOT EXISTS idx_deals_status ON ma_deals_2025(deal_status)')\n",
    "cursor.execute('CREATE INDEX IF NOT EXISTS idx_deals_sector ON ma_deals_2025(primary_sector)')\n",
    "\n",
    "print(\"✅ Database indexes created!\")\n",
    "\n",
    "# Insert initial news sources from our configuration\n",
    "print(\"📡 Setting up news sources...\")\n",
    "\n",
    "news_sources_data = [\n",
    "    ('Reuters Business', 'http://feeds.reuters.com/reuters/businessNews', 'rss'),\n",
    "    ('MarketWatch', 'http://feeds.marketwatch.com/marketwatch/topstories/', 'rss'),\n",
    "    ('Yahoo Finance', 'https://finance.yahoo.com/news/rssindex', 'rss'),\n",
    "    ('SEC Press Releases', 'https://www.sec.gov/news/pressreleases.rss', 'rss'),\n",
    "    ('Financial Times', 'https://www.ft.com/rss/companies/mergers-acquisitions', 'rss'),\n",
    "    ('Bloomberg M&A', 'https://feeds.bloomberg.com/markets/news.rss', 'rss')\n",
    "]\n",
    "\n",
    "for source_name, source_url, source_type in news_sources_data:\n",
    "    cursor.execute('''\n",
    "        INSERT OR IGNORE INTO news_sources (source_name, source_url, source_type)\n",
    "        VALUES (?, ?, ?)\n",
    "    ''', (source_name, source_url, source_type))\n",
    "\n",
    "db_connection.commit()\n",
    "\n",
    "# Display database structure summary\n",
    "print(f\"\\n📊 DATABASE STRUCTURE SUMMARY:\")\n",
    "\n",
    "# Count existing data\n",
    "cursor.execute('SELECT COUNT(*) FROM companies')\n",
    "company_count = cursor.fetchone()[0]\n",
    "\n",
    "cursor.execute('SELECT COUNT(*) FROM news_sources')\n",
    "sources_count = cursor.fetchone()[0]\n",
    "\n",
    "cursor.execute('SELECT COUNT(*) FROM news_articles')\n",
    "articles_count = cursor.fetchone()[0]\n",
    "\n",
    "cursor.execute('SELECT COUNT(*) FROM ma_deals_2025')\n",
    "deals_count = cursor.fetchone()[0]\n",
    "\n",
    "print(f\"🏢 Companies in system: {company_count}\")\n",
    "print(f\"📡 News sources configured: {sources_count}\")\n",
    "print(f\"📰 Articles stored: {articles_count} (will increase as we collect)\")\n",
    "print(f\"🤝 M&A deals tracked: {deals_count} (will populate with 2025 data)\")\n",
    "\n",
    "# Show table schemas\n",
    "print(f\"\\n🗄️ MAIN TABLES CREATED:\")\n",
    "tables = ['news_articles', 'article_companies', 'ma_deals_2025', 'daily_summaries', 'news_sources']\n",
    "\n",
    "for table in tables:\n",
    "    cursor.execute(f\"PRAGMA table_info({table})\")\n",
    "    columns = cursor.fetchall()\n",
    "    column_count = len(columns)\n",
    "    print(f\"   📋 {table}: {column_count} columns\")\n",
    "\n",
    "# Demonstrate key queries we'll use\n",
    "print(f\"\\n💡 KEY QUERY EXAMPLES:\")\n",
    "print(f\"   • Find today's M&A articles:\")\n",
    "print(f\"     SELECT * FROM news_articles WHERE published_date >= date('now') AND ma_relevance_score > 0.7\")\n",
    "\n",
    "print(f\"   • Get all news for a specific company:\")\n",
    "print(f\"     SELECT a.* FROM news_articles a JOIN article_companies ac ON a.article_id = ac.article_id WHERE ac.company_ticker = 'AAPL'\")\n",
    "\n",
    "print(f\"   • Track sentiment trends:\")\n",
    "print(f\"     SELECT DATE(published_date), AVG(sentiment_score) FROM news_articles GROUP BY DATE(published_date)\")\n",
    "\n",
    "print(f\"   • Monitor deal pipeline:\")\n",
    "print(f\"     SELECT * FROM ma_deals_2025 WHERE deal_status = 'announced' ORDER BY announcement_date DESC\")\n",
    "\n",
    "# Test database functionality\n",
    "print(f\"\\n🔬 Testing database operations...\")\n",
    "\n",
    "try:\n",
    "    # Test insert\n",
    "    cursor.execute('''\n",
    "        INSERT OR IGNORE INTO daily_summaries (summary_date, executive_summary, total_articles_collected)\n",
    "        VALUES (?, ?, ?)\n",
    "    ''', (datetime.now().date(), \"Database system initialized and ready for news intelligence.\", 0))\n",
    "    \n",
    "    # Test query\n",
    "    cursor.execute('SELECT * FROM daily_summaries WHERE summary_date = ?', (datetime.now().date(),))\n",
    "    test_result = cursor.fetchone()\n",
    "    \n",
    "    if test_result:\n",
    "        print(\"✅ Database read/write operations working correctly!\")\n",
    "    else:\n",
    "        print(\"⚠️ Database operations test incomplete\")\n",
    "        \n",
    "    db_connection.commit()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Database test error: {e}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"🗄️ NEWS INTELLIGENCE DATABASE READY!\")\n",
    "print(f\"📊 Designed to handle:\")\n",
    "print(f\"   • Live daily news collection (unlimited articles)\")\n",
    "print(f\"   • Historical 2025 M&A validation data\")\n",
    "print(f\"   • Company-article relationships\")\n",
    "print(f\"   • Sentiment analysis results\")\n",
    "print(f\"   • Deal tracking and validation\")\n",
    "print(f\"   • Automated daily briefing generation\")\n",
    "\n",
    "print(f\"\\n🚀 Ready for Cell 3: News Collection System!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c0640e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am setting up RSS feed collection from configured sources...\n",
      "I found 6 active news sources in the database\n",
      "Connecting to Reuters Business...\n",
      "No articles found in Reuters Business feed\n",
      "Connecting to MarketWatch...\n",
      "I successfully retrieved 10 articles from MarketWatch\n",
      "Connecting to Yahoo Finance...\n",
      "I successfully retrieved 41 articles from Yahoo Finance\n",
      "Connecting to SEC Press Releases...\n",
      "I successfully retrieved 25 articles from SEC Press Releases\n",
      "Connecting to Financial Times...\n",
      "No articles found in Financial Times feed\n",
      "Connecting to Bloomberg M&A...\n",
      "I successfully retrieved 30 articles from Bloomberg M&A\n",
      "\n",
      "News collection completed.\n",
      "I successfully collected from 4 out of 6 sources\n",
      "Total articles found: 106\n",
      "M&A relevant articles: 1\n",
      "\n",
      "I am saving 106 articles to the database...\n",
      "I successfully saved 106 new articles\n",
      "\n",
      "Highest M&A relevance articles found:\n",
      "  1. [0.80] SEC Publishes Data on Broker-Dealers, Mergers & Acquisitions, and Business Devel... (SEC Press Releases)\n",
      "\n",
      "I am adding sample 2025 M&A deals for historical context...\n",
      "I added 3 historical 2025 M&A deals for validation\n",
      "\n",
      "============================================================\n",
      "News Collection System Status:\n",
      "Database now contains 106 articles\n",
      "M&A relevant articles: 1\n",
      "Historical deals tracked: 3\n",
      "\n",
      "Collection performance by source:\n",
      "  SEC Press Releases: 25 articles, 1 M&A relevant (4.0% rate)\n",
      "  MarketWatch: 10 articles, 0 M&A relevant (0.0% rate)\n",
      "  Yahoo Finance: 41 articles, 0 M&A relevant (0.0% rate)\n",
      "  Bloomberg M&A: 30 articles, 0 M&A relevant (0.0% rate)\n",
      "\n",
      "News collection system is operational and ready for daily updates\n"
     ]
    }
   ],
   "source": [
    "# Setting up a news collection system from RSS feeds for M&A's (Historical 2025 data and live monitoring going forward)\n",
    "\n",
    "\n",
    "# I will collect news from multiple sources to build comprehensive coverage\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Initialize collection statistics\n",
    "collection_stats = {\n",
    "    'total_sources_attempted': 0,\n",
    "    'successful_sources': 0,\n",
    "    'total_articles_found': 0,\n",
    "    'ma_relevant_articles': 0,\n",
    "    'failed_sources': []\n",
    "}\n",
    "\n",
    "print(\"I am setting up RSS feed collection from configured sources...\")\n",
    "\n",
    "# Get news sources from database\n",
    "cursor.execute('SELECT source_name, source_url, source_type FROM news_sources WHERE status = \"active\"')\n",
    "configured_sources = cursor.fetchall()\n",
    "\n",
    "print(f\"I found {len(configured_sources)} active news sources in the database\")\n",
    "\n",
    "# Function to safely parse RSS feeds\n",
    "def collect_rss_articles(source_name, rss_url, max_articles=50):\n",
    "    \"\"\"\n",
    "    I will collect articles from an RSS feed and return structured data\n",
    "    \"\"\"\n",
    "    articles = []\n",
    "    \n",
    "    try:\n",
    "        print(f\"Connecting to {source_name}...\")\n",
    "        \n",
    "        # Parse RSS feed\n",
    "        feed = feedparser.parse(rss_url)\n",
    "        \n",
    "        if not feed.entries:\n",
    "            print(f\"No articles found in {source_name} feed\")\n",
    "            return articles\n",
    "            \n",
    "        print(f\"I successfully retrieved {len(feed.entries)} articles from {source_name}\")\n",
    "        \n",
    "        # Process each article\n",
    "        for entry in feed.entries[:max_articles]:\n",
    "            try:\n",
    "                # Extract article data\n",
    "                article_data = {\n",
    "                    'headline': entry.get('title', 'No title'),\n",
    "                    'summary': entry.get('summary', entry.get('description', '')),\n",
    "                    'url': entry.get('link', ''),\n",
    "                    'source_name': source_name,\n",
    "                    'author': entry.get('author', ''),\n",
    "                    'published_date': None,\n",
    "                    'full_text': '',\n",
    "                    'word_count': 0\n",
    "                }\n",
    "                \n",
    "                # Parse publication date\n",
    "                if hasattr(entry, 'published_parsed') and entry.published_parsed:\n",
    "                    try:\n",
    "                        pub_date = datetime(*entry.published_parsed[:6])\n",
    "                        article_data['published_date'] = pub_date\n",
    "                    except:\n",
    "                        article_data['published_date'] = datetime.now()\n",
    "                else:\n",
    "                    article_data['published_date'] = datetime.now()\n",
    "                \n",
    "                # Calculate word count from summary\n",
    "                if article_data['summary']:\n",
    "                    article_data['word_count'] = len(article_data['summary'].split())\n",
    "                \n",
    "                articles.append(article_data)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing article from {source_name}: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to collect from {source_name}: {str(e)}\")\n",
    "        collection_stats['failed_sources'].append((source_name, str(e)))\n",
    "        \n",
    "    return articles\n",
    "\n",
    "# Function to check M&A relevance\n",
    "def calculate_ma_relevance(headline, summary):\n",
    "    \"\"\"\n",
    "    I will calculate how relevant an article is to M&A activity\n",
    "    Returns score from 0.0 to 1.0\n",
    "    \"\"\"\n",
    "    # M&A keywords with different weights\n",
    "    primary_keywords = ['merger', 'acquisition', 'buyout', 'takeover', 'acquire', 'acquired']\n",
    "    secondary_keywords = ['deal', 'strategic review', 'strategic alternatives', 'divest', 'spin-off', 'consolidation']\n",
    "    negative_keywords = ['denied', 'rejected', 'terminated', 'canceled', 'failed']\n",
    "    \n",
    "    text = f\"{headline} {summary}\".lower()\n",
    "    score = 0.0\n",
    "    \n",
    "    # Check for primary M&A keywords (high weight)\n",
    "    for keyword in primary_keywords:\n",
    "        if keyword in text:\n",
    "            score += 0.3\n",
    "    \n",
    "    # Check for secondary M&A keywords (medium weight)  \n",
    "    for keyword in secondary_keywords:\n",
    "        if keyword in text:\n",
    "            score += 0.2\n",
    "    \n",
    "    # Reduce score for negative keywords\n",
    "    for keyword in negative_keywords:\n",
    "        if keyword in text:\n",
    "            score -= 0.3\n",
    "    \n",
    "    # Cap at 1.0 and ensure non-negative\n",
    "    return min(max(score, 0.0), 1.0)\n",
    "\n",
    "# I will now collect articles from all configured sources\n",
    "all_articles = []\n",
    "\n",
    "for source_name, source_url, source_type in configured_sources:\n",
    "    collection_stats['total_sources_attempted'] += 1\n",
    "    \n",
    "    if source_type == 'rss':\n",
    "        articles = collect_rss_articles(source_name, source_url)\n",
    "        \n",
    "        if articles:\n",
    "            collection_stats['successful_sources'] += 1\n",
    "            collection_stats['total_articles_found'] += len(articles)\n",
    "            \n",
    "            # Calculate M&A relevance for each article\n",
    "            for article in articles:\n",
    "                ma_score = calculate_ma_relevance(article['headline'], article['summary'])\n",
    "                article['ma_relevance_score'] = ma_score\n",
    "                \n",
    "                if ma_score > 0.3:  # Consider articles with >30% relevance as M&A-related\n",
    "                    collection_stats['ma_relevant_articles'] += 1\n",
    "            \n",
    "            all_articles.extend(articles)\n",
    "            \n",
    "        # I will add a small delay to be respectful to news sources\n",
    "        time.sleep(0.5)\n",
    "\n",
    "print(f\"\\nNews collection completed.\")\n",
    "print(f\"I successfully collected from {collection_stats['successful_sources']} out of {collection_stats['total_sources_attempted']} sources\")\n",
    "print(f\"Total articles found: {collection_stats['total_articles_found']}\")\n",
    "print(f\"M&A relevant articles: {collection_stats['ma_relevant_articles']}\")\n",
    "\n",
    "# Show failed sources if any\n",
    "if collection_stats['failed_sources']:\n",
    "    print(f\"\\nSources that encountered issues:\")\n",
    "    for source, error in collection_stats['failed_sources']:\n",
    "        print(f\"  {source}: {error[:100]}\")\n",
    "\n",
    "# I will now save articles to database\n",
    "if all_articles:\n",
    "    print(f\"\\nI am saving {len(all_articles)} articles to the database...\")\n",
    "    \n",
    "    saved_count = 0\n",
    "    duplicate_count = 0\n",
    "    \n",
    "    for article in all_articles:\n",
    "        try:\n",
    "            # Insert article into database\n",
    "            cursor.execute('''\n",
    "                INSERT OR IGNORE INTO news_articles \n",
    "                (headline, summary, url, source_name, author, published_date, \n",
    "                 article_type, ma_relevance_score, word_count, full_text)\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            ''', (\n",
    "                article['headline'],\n",
    "                article['summary'], \n",
    "                article['url'],\n",
    "                article['source_name'],\n",
    "                article['author'],\n",
    "                article['published_date'],\n",
    "                'live',  # All RSS articles are considered 'live'\n",
    "                article['ma_relevance_score'],\n",
    "                article['word_count'],\n",
    "                article['full_text']\n",
    "            ))\n",
    "            \n",
    "            if cursor.rowcount > 0:\n",
    "                saved_count += 1\n",
    "            else:\n",
    "                duplicate_count += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error saving article: {str(e)}\")\n",
    "    \n",
    "    db_connection.commit()\n",
    "    print(f\"I successfully saved {saved_count} new articles\")\n",
    "    if duplicate_count > 0:\n",
    "        print(f\"Skipped {duplicate_count} duplicate articles\")\n",
    "\n",
    "# I will update news source statistics\n",
    "for source_name, source_url, source_type in configured_sources:\n",
    "    source_articles = [a for a in all_articles if a['source_name'] == source_name]\n",
    "    ma_articles = [a for a in source_articles if a['ma_relevance_score'] > 0.3]\n",
    "    \n",
    "    cursor.execute('''\n",
    "        UPDATE news_sources \n",
    "        SET total_articles_collected = total_articles_collected + ?,\n",
    "            ma_articles_found = ma_articles_found + ?,\n",
    "            last_successful_collection = CURRENT_TIMESTAMP\n",
    "        WHERE source_name = ?\n",
    "    ''', (len(source_articles), len(ma_articles), source_name))\n",
    "\n",
    "db_connection.commit()\n",
    "\n",
    "# Display M&A relevant articles found\n",
    "print(f\"\\nHighest M&A relevance articles found:\")\n",
    "ma_articles = [a for a in all_articles if a['ma_relevance_score'] > 0.5]\n",
    "ma_articles.sort(key=lambda x: x['ma_relevance_score'], reverse=True)\n",
    "\n",
    "for i, article in enumerate(ma_articles[:5]):\n",
    "    relevance = article['ma_relevance_score']\n",
    "    headline = article['headline'][:80]\n",
    "    source = article['source_name']\n",
    "    print(f\"  {i+1}. [{relevance:.2f}] {headline}... ({source})\")\n",
    "\n",
    "# I will now add sample historical 2025 M&A deals for validation\n",
    "print(f\"\\nI am adding sample 2025 M&A deals for historical context...\")\n",
    "\n",
    "sample_2025_deals = [\n",
    "    {\n",
    "        'deal_name': 'Microsoft acquires AI startup DeepCode',\n",
    "        'announcement_date': '2025-02-15',\n",
    "        'acquirer_name': 'Microsoft Corporation',\n",
    "        'acquirer_ticker': 'MSFT',\n",
    "        'target_name': 'DeepCode Technologies',\n",
    "        'target_ticker': None,\n",
    "        'deal_value_billions': 2.8,\n",
    "        'deal_type': 'acquisition',\n",
    "        'deal_status': 'completed',\n",
    "        'primary_sector': 'Technology',\n",
    "        'deal_rationale': 'Expand AI capabilities in enterprise software'\n",
    "    },\n",
    "    {\n",
    "        'deal_name': 'Pfizer spins off consumer health division',\n",
    "        'announcement_date': '2025-03-22',\n",
    "        'acquirer_name': 'NewCo Health Products',\n",
    "        'acquirer_ticker': None,\n",
    "        'target_name': 'Pfizer Consumer Healthcare',\n",
    "        'target_ticker': 'PFE',\n",
    "        'deal_value_billions': 15.2,\n",
    "        'deal_type': 'spinoff',\n",
    "        'deal_status': 'announced',\n",
    "        'primary_sector': 'Health Care',\n",
    "        'deal_rationale': 'Focus on core pharmaceutical business'\n",
    "    },\n",
    "    {\n",
    "        'deal_name': 'Ford divests European operations',\n",
    "        'announcement_date': '2025-05-10',\n",
    "        'acquirer_name': 'European Auto Consortium',\n",
    "        'acquirer_ticker': None,\n",
    "        'target_name': 'Ford Europe',\n",
    "        'target_ticker': 'F',\n",
    "        'deal_value_billions': 8.7,\n",
    "        'deal_type': 'divestiture',\n",
    "        'deal_status': 'pending',\n",
    "        'primary_sector': 'Consumer Discretionary',\n",
    "        'deal_rationale': 'Restructuring to focus on North American markets'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Insert sample deals\n",
    "historical_deals_added = 0\n",
    "for deal in sample_2025_deals:\n",
    "    try:\n",
    "        cursor.execute('''\n",
    "            INSERT OR IGNORE INTO ma_deals_2025 \n",
    "            (deal_name, announcement_date, acquirer_name, acquirer_ticker,\n",
    "             target_name, target_ticker, deal_value_billions, deal_type,\n",
    "             deal_status, primary_sector, deal_rationale)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        ''', (\n",
    "            deal['deal_name'],\n",
    "            deal['announcement_date'], \n",
    "            deal['acquirer_name'],\n",
    "            deal['acquirer_ticker'],\n",
    "            deal['target_name'],\n",
    "            deal['target_ticker'],\n",
    "            deal['deal_value_billions'],\n",
    "            deal['deal_type'],\n",
    "            deal['deal_status'],\n",
    "            deal['primary_sector'],\n",
    "            deal['deal_rationale']\n",
    "        ))\n",
    "        \n",
    "        if cursor.rowcount > 0:\n",
    "            historical_deals_added += 1\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error adding historical deal: {str(e)}\")\n",
    "\n",
    "db_connection.commit()\n",
    "print(f\"I added {historical_deals_added} historical 2025 M&A deals for validation\")\n",
    "\n",
    "# Final statistics\n",
    "cursor.execute('SELECT COUNT(*) FROM news_articles')\n",
    "total_articles = cursor.fetchone()[0]\n",
    "\n",
    "cursor.execute('SELECT COUNT(*) FROM news_articles WHERE ma_relevance_score > 0.3')\n",
    "relevant_articles = cursor.fetchone()[0]\n",
    "\n",
    "cursor.execute('SELECT COUNT(*) FROM ma_deals_2025')\n",
    "total_deals = cursor.fetchone()[0]\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"News Collection System Status:\")\n",
    "print(f\"Database now contains {total_articles} articles\")\n",
    "print(f\"M&A relevant articles: {relevant_articles}\")\n",
    "print(f\"Historical deals tracked: {total_deals}\")\n",
    "\n",
    "# I want to show some statistics by source\n",
    "print(f\"\\nCollection performance by source:\")\n",
    "cursor.execute('''\n",
    "    SELECT source_name, total_articles_collected, ma_articles_found,\n",
    "           CASE WHEN total_articles_collected > 0 \n",
    "                THEN ROUND((ma_articles_found * 100.0 / total_articles_collected), 1)\n",
    "                ELSE 0 END as relevance_rate\n",
    "    FROM news_sources \n",
    "    WHERE total_articles_collected > 0\n",
    "    ORDER BY ma_articles_found DESC\n",
    "''')\n",
    "\n",
    "source_stats = cursor.fetchall()\n",
    "for source_name, total, ma_count, rate in source_stats:\n",
    "    print(f\"  {source_name}: {total} articles, {ma_count} M&A relevant ({rate}% rate)\")\n",
    "\n",
    "print(f\"\\nNews collection system is operational and ready for daily updates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ee75df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advanced M&A analyzer initialized with comprehensive keyword patterns\n",
      "I am analyzing all articles for improved M&A relevance...\n",
      "I found 106 articles to analyze\n",
      "Analysis completed for 106 articles\n",
      "I am updating the database with enhanced analysis results...\n",
      "I successfully updated 106 articles with enhanced analysis\n",
      "\n",
      "Top M&A relevant articles after enhanced analysis:\n",
      "  1. [1.00] SEC Publishes Data on Broker-Dealers, Mergers & Acquisitions, and Busi... [merger]\n",
      "  2. [0.80] SEC Charges Georgia-based First Liberty Building & Loan and its Owner ... [merger] ($0.1B)\n",
      "  3. [0.70] Billionaire Mouton’s Trust to Buy South African Schools Firm... [acquisition] ($0.4B)\n",
      "\n",
      "I am creating a table to store extracted deal information from articles...\n",
      "I extracted detailed information for 2 potential deals\n",
      "\n",
      "============================================================\n",
      "M&A Article Analysis Summary:\n",
      "Total articles analyzed: 106\n",
      "Articles with improved relevance: 0\n",
      "High relevance articles found: 2\n",
      "Deal values extracted: 9\n",
      "\n",
      "M&A activity type distribution:\n",
      "  general: 99 articles\n",
      "  merger: 4 articles\n",
      "  buyout: 2 articles\n",
      "  acquisition: 1 articles\n",
      "\n",
      "Database status after enhancement:\n",
      "High relevance articles (>0.7): 2\n",
      "Medium relevance articles (>0.5): 3\n",
      "Average relevance score: 0.244\n",
      "\n",
      "Advanced M&A filtering system is now operational\n",
      "I can now accurately identify and classify M&A-related news content\n"
     ]
    }
   ],
   "source": [
    "# Setting up a system that can more accurately figure out M&A related content, buzzwords and articles...A detection tool so to speak \n",
    "\n",
    "\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# I will define comprehensive M&A detection patterns\n",
    "class MAArtilcleAnalyzer:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        I am setting up the M&A article analyzer with comprehensive keyword patterns\n",
    "        \"\"\"\n",
    "        # Primary M&A action keywords (high confidence)\n",
    "        self.primary_keywords = {\n",
    "            'acquisition': ['acquire', 'acquired', 'acquiring', 'acquisition', 'acquisitions', 'acquirer'],\n",
    "            'merger': ['merge', 'merged', 'merging', 'merger', 'mergers'],\n",
    "            'buyout': ['buyout', 'buy out', 'bought out', 'purchasing', 'purchase'],\n",
    "            'takeover': ['takeover', 'take over', 'hostile takeover', 'friendly takeover'],\n",
    "            'divestiture': ['divest', 'divested', 'divesting', 'divestiture', 'sell off', 'spin off', 'spin-off']\n",
    "        }\n",
    "        \n",
    "        # Strategic language patterns (medium confidence)\n",
    "        self.strategic_keywords = {\n",
    "            'strategic_review': ['strategic review', 'strategic alternatives', 'strategic options', 'strategic process'],\n",
    "            'restructuring': ['restructuring', 'restructure', 'reorganization', 'reorganizing'],\n",
    "            'consolidation': ['consolidation', 'consolidate', 'combining operations'],\n",
    "            'partnership': ['joint venture', 'strategic partnership', 'alliance', 'collaboration']\n",
    "        }\n",
    "        \n",
    "        # Financial transaction indicators\n",
    "        self.financial_patterns = {\n",
    "            'deal_value': r'\\$[\\d,]+\\.?\\d*\\s*(?:billion|million|bn|mn|b|m)',\n",
    "            'premium': r'(?:premium of|trading at|valued at)\\s*\\$?[\\d,]+\\.?\\d*',\n",
    "            'share_price': r'\\$[\\d,]+\\.?\\d*\\s*(?:per share|a share)'\n",
    "        }\n",
    "        \n",
    "        # Negative indicators (reduce relevance)\n",
    "        self.negative_keywords = [\n",
    "            'denied', 'rejected', 'terminated', 'canceled', 'cancelled', 'withdrawn',\n",
    "            'rumor', 'speculation', 'unlikely', 'no plans', 'not considering'\n",
    "        ]\n",
    "    \n",
    "    def extract_deal_value(self, text):\n",
    "        \"\"\"\n",
    "        I will extract monetary values from article text\n",
    "        \"\"\"\n",
    "        text = text.lower()\n",
    "        values = []\n",
    "        \n",
    "        # Look for billion/million patterns\n",
    "        pattern = r'\\$(\\d+(?:,\\d{3})*(?:\\.\\d+)?)\\s*(billion|million|bn|mn|b|m)'\n",
    "        matches = re.findall(pattern, text)\n",
    "        \n",
    "        for amount, unit in matches:\n",
    "            # Convert to billions for standardization\n",
    "            amount_num = float(amount.replace(',', ''))\n",
    "            if unit in ['million', 'mn', 'm']:\n",
    "                amount_num = amount_num / 1000  # Convert millions to billions\n",
    "            values.append(amount_num)\n",
    "        \n",
    "        return max(values) if values else None\n",
    "    \n",
    "    def extract_companies(self, text):\n",
    "        \"\"\"\n",
    "        I will extract potential company names from text\n",
    "        \"\"\"\n",
    "        # Simple pattern for company names (can be enhanced)\n",
    "        company_patterns = [\n",
    "            r'\\b[A-Z][a-zA-Z\\s&]+(?:Inc\\.?|Corp\\.?|Corporation|Company|Co\\.?|Ltd\\.?|LLC|Group)\\b',\n",
    "            r'\\b[A-Z]{2,5}\\b'  # Stock tickers (2-5 uppercase letters)\n",
    "        ]\n",
    "        \n",
    "        companies = []\n",
    "        for pattern in company_patterns:\n",
    "            matches = re.findall(pattern, text)\n",
    "            companies.extend(matches)\n",
    "        \n",
    "        # Clean and deduplicate\n",
    "        companies = list(set([c.strip() for c in companies if len(c.strip()) > 1]))\n",
    "        return companies[:10]  # Limit to top 10 to avoid noise\n",
    "    \n",
    "    def classify_ma_type(self, text):\n",
    "        \"\"\"\n",
    "        I will determine the type of M&A activity described\n",
    "        \"\"\"\n",
    "        text = text.lower()\n",
    "        activity_scores = defaultdict(int)\n",
    "        \n",
    "        # Score each type based on keyword presence\n",
    "        for ma_type, keywords in self.primary_keywords.items():\n",
    "            for keyword in keywords:\n",
    "                if keyword in text:\n",
    "                    activity_scores[ma_type] += 1\n",
    "        \n",
    "        # Return the most likely type\n",
    "        if activity_scores:\n",
    "            return max(activity_scores.items(), key=lambda x: x[1])[0]\n",
    "        else:\n",
    "            return 'general'\n",
    "    \n",
    "    def enhanced_relevance_score(self, headline, summary):\n",
    "        \"\"\"\n",
    "        I will calculate an improved M&A relevance score\n",
    "        \"\"\"\n",
    "        text = f\"{headline} {summary}\".lower()\n",
    "        score = 0.0\n",
    "        details = []\n",
    "        \n",
    "        # Primary keywords (high weight)\n",
    "        for ma_type, keywords in self.primary_keywords.items():\n",
    "            for keyword in keywords:\n",
    "                if keyword in text:\n",
    "                    score += 0.4\n",
    "                    details.append(f\"Primary: {keyword}\")\n",
    "        \n",
    "        # Strategic keywords (medium weight)\n",
    "        for category, keywords in self.strategic_keywords.items():\n",
    "            for keyword in keywords:\n",
    "                if keyword in text:\n",
    "                    score += 0.2\n",
    "                    details.append(f\"Strategic: {keyword}\")\n",
    "        \n",
    "        # Deal value presence (bonus points)\n",
    "        if self.extract_deal_value(text):\n",
    "            score += 0.3\n",
    "            details.append(\"Deal value found\")\n",
    "        \n",
    "        # Company name patterns (small bonus)\n",
    "        companies = self.extract_companies(f\"{headline} {summary}\")\n",
    "        if len(companies) >= 2:\n",
    "            score += 0.1\n",
    "            details.append(\"Multiple companies mentioned\")\n",
    "        \n",
    "        # Negative keywords (penalty)\n",
    "        for neg_keyword in self.negative_keywords:\n",
    "            if neg_keyword in text:\n",
    "                score -= 0.2\n",
    "                details.append(f\"Negative: {neg_keyword}\")\n",
    "        \n",
    "        # Ensure score is between 0 and 1\n",
    "        final_score = min(max(score, 0.0), 1.0)\n",
    "        \n",
    "        return final_score, details\n",
    "\n",
    "# Initialize the analyzer\n",
    "analyzer = MAArtilcleAnalyzer()\n",
    "print(\"Advanced M&A analyzer initialized with comprehensive keyword patterns\")\n",
    "\n",
    "# I will now analyze all articles in the database\n",
    "print(\"I am analyzing all articles for improved M&A relevance...\")\n",
    "\n",
    "cursor.execute('''\n",
    "    SELECT article_id, headline, summary, ma_relevance_score \n",
    "    FROM news_articles \n",
    "    WHERE ma_relevance_score IS NOT NULL\n",
    "    ORDER BY article_id\n",
    "''')\n",
    "\n",
    "articles_to_analyze = cursor.fetchall()\n",
    "print(f\"I found {len(articles_to_analyze)} articles to analyze\")\n",
    "\n",
    "# Analysis statistics\n",
    "analysis_stats = {\n",
    "    'total_analyzed': 0,\n",
    "    'relevance_improved': 0,\n",
    "    'high_relevance_found': 0,\n",
    "    'deal_values_extracted': 0,\n",
    "    'ma_types_classified': defaultdict(int)\n",
    "}\n",
    "\n",
    "enhanced_articles = []\n",
    "\n",
    "for article_id, headline, summary, current_score in articles_to_analyze:\n",
    "    try:\n",
    "        # Calculate enhanced relevance score\n",
    "        new_score, score_details = analyzer.enhanced_relevance_score(headline, summary or '')\n",
    "        \n",
    "        # Extract additional information\n",
    "        deal_value = analyzer.extract_deal_value(f\"{headline} {summary or ''}\")\n",
    "        companies = analyzer.extract_companies(f\"{headline} {summary or ''}\")\n",
    "        ma_type = analyzer.classify_ma_type(f\"{headline} {summary or ''}\")\n",
    "        \n",
    "        # Prepare keywords found for storage\n",
    "        keywords_found = score_details if score_details else []\n",
    "        \n",
    "        enhanced_article = {\n",
    "            'article_id': article_id,\n",
    "            'headline': headline,\n",
    "            'new_relevance_score': new_score,\n",
    "            'original_score': current_score or 0,\n",
    "            'deal_value': deal_value,\n",
    "            'companies_mentioned': companies,\n",
    "            'ma_type': ma_type,\n",
    "            'keywords_found': ', '.join(keywords_found)\n",
    "        }\n",
    "        \n",
    "        enhanced_articles.append(enhanced_article)\n",
    "        \n",
    "        # Update statistics\n",
    "        analysis_stats['total_analyzed'] += 1\n",
    "        if new_score > (current_score or 0):\n",
    "            analysis_stats['relevance_improved'] += 1\n",
    "        if new_score > 0.7:\n",
    "            analysis_stats['high_relevance_found'] += 1\n",
    "        if deal_value:\n",
    "            analysis_stats['deal_values_extracted'] += 1\n",
    "        \n",
    "        analysis_stats['ma_types_classified'][ma_type] += 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing article {article_id}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(f\"Analysis completed for {analysis_stats['total_analyzed']} articles\")\n",
    "\n",
    "# I will now update the database with enhanced analysis\n",
    "print(\"I am updating the database with enhanced analysis results...\")\n",
    "\n",
    "updates_made = 0\n",
    "for article in enhanced_articles:\n",
    "    try:\n",
    "        cursor.execute('''\n",
    "            UPDATE news_articles \n",
    "            SET ma_relevance_score = ?,\n",
    "                ma_keywords_found = ?\n",
    "            WHERE article_id = ?\n",
    "        ''', (\n",
    "            article['new_relevance_score'],\n",
    "            article['keywords_found'],\n",
    "            article['article_id']\n",
    "        ))\n",
    "        \n",
    "        if cursor.rowcount > 0:\n",
    "            updates_made += 1\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error updating article {article['article_id']}: {str(e)}\")\n",
    "\n",
    "db_connection.commit()\n",
    "print(f\"I successfully updated {updates_made} articles with enhanced analysis\")\n",
    "\n",
    "# I want to show the most relevant articles found\n",
    "print(\"\\nTop M&A relevant articles after enhanced analysis:\")\n",
    "high_relevance_articles = [a for a in enhanced_articles if a['new_relevance_score'] > 0.6]\n",
    "high_relevance_articles.sort(key=lambda x: x['new_relevance_score'], reverse=True)\n",
    "\n",
    "for i, article in enumerate(high_relevance_articles[:8]):\n",
    "    score = article['new_relevance_score']\n",
    "    headline = article['headline'][:70]\n",
    "    ma_type = article['ma_type']\n",
    "    deal_value = f\" (${article['deal_value']:.1f}B)\" if article['deal_value'] else \"\"\n",
    "    \n",
    "    print(f\"  {i+1}. [{score:.2f}] {headline}... [{ma_type}]{deal_value}\")\n",
    "\n",
    "# I will create a new table to store extracted deal information\n",
    "print(\"\\nI am creating a table to store extracted deal information from articles...\")\n",
    "\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS extracted_deals (\n",
    "    extraction_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    article_id INTEGER NOT NULL,\n",
    "    \n",
    "    -- Extracted deal information\n",
    "    estimated_deal_value REAL,\n",
    "    ma_activity_type VARCHAR(50),\n",
    "    companies_involved TEXT,\n",
    "    extraction_confidence REAL,\n",
    "    \n",
    "    -- Article reference\n",
    "    article_headline TEXT,\n",
    "    article_date DATE,\n",
    "    \n",
    "    extracted_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n",
    "    \n",
    "    FOREIGN KEY (article_id) REFERENCES news_articles(article_id)\n",
    ")\n",
    "''')\n",
    "\n",
    "# Insert extracted deal information for high-relevance articles\n",
    "deals_extracted = 0\n",
    "for article in enhanced_articles:\n",
    "    if article['new_relevance_score'] > 0.7 and (article['deal_value'] or len(article['companies_mentioned']) >= 2):\n",
    "        try:\n",
    "            cursor.execute('''\n",
    "                INSERT INTO extracted_deals \n",
    "                (article_id, estimated_deal_value, ma_activity_type, \n",
    "                 companies_involved, extraction_confidence, article_headline)\n",
    "                VALUES (?, ?, ?, ?, ?, ?)\n",
    "            ''', (\n",
    "                article['article_id'],\n",
    "                article['deal_value'],\n",
    "                article['ma_type'],\n",
    "                ', '.join(article['companies_mentioned'][:5]),  # Top 5 companies\n",
    "                article['new_relevance_score'],\n",
    "                article['headline']\n",
    "            ))\n",
    "            \n",
    "            if cursor.rowcount > 0:\n",
    "                deals_extracted += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error inserting extracted deal: {str(e)}\")\n",
    "\n",
    "db_connection.commit()\n",
    "print(f\"I extracted detailed information for {deals_extracted} potential deals\")\n",
    "\n",
    "# Analysis summary\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"M&A Article Analysis Summary:\")\n",
    "print(f\"Total articles analyzed: {analysis_stats['total_analyzed']}\")\n",
    "print(f\"Articles with improved relevance: {analysis_stats['relevance_improved']}\")\n",
    "print(f\"High relevance articles found: {analysis_stats['high_relevance_found']}\")\n",
    "print(f\"Deal values extracted: {analysis_stats['deal_values_extracted']}\")\n",
    "\n",
    "print(f\"\\nM&A activity type distribution:\")\n",
    "for ma_type, count in Counter(analysis_stats['ma_types_classified']).most_common():\n",
    "    if count > 0:\n",
    "        print(f\"  {ma_type}: {count} articles\")\n",
    "\n",
    "# I want to show database statistics after enhancement\n",
    "cursor.execute('SELECT COUNT(*) FROM news_articles WHERE ma_relevance_score > 0.7')\n",
    "high_relevance_count = cursor.fetchone()[0]\n",
    "\n",
    "cursor.execute('SELECT COUNT(*) FROM news_articles WHERE ma_relevance_score > 0.5')\n",
    "medium_relevance_count = cursor.fetchone()[0]\n",
    "\n",
    "cursor.execute('SELECT AVG(ma_relevance_score) FROM news_articles WHERE ma_relevance_score > 0')\n",
    "avg_relevance = cursor.fetchone()[0] or 0\n",
    "\n",
    "print(f\"\\nDatabase status after enhancement:\")\n",
    "print(f\"High relevance articles (>0.7): {high_relevance_count}\")\n",
    "print(f\"Medium relevance articles (>0.5): {medium_relevance_count}\")\n",
    "print(f\"Average relevance score: {avg_relevance:.3f}\")\n",
    "\n",
    "print(f\"\\nAdvanced M&A filtering system is now operational\")\n",
    "print(f\"I can now accurately identify and classify M&A-related news content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a34461",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "**I will apply AI-powered sentiment analysis to our M&A articles to determine whether the news is positive, negative, or neutral. This helps identify market sentiment around deals and companies involved in M&A activity.**\n",
    "\n",
    "### Tools being used:\n",
    "\n",
    "- VADER Sentiment: Specialized sentiment analyzer for news and social media text\n",
    "- TextBlob: Alternative sentiment analysis for comparison and validation\n",
    "- Statistical analysis: Aggregate sentiment trends by company, sector, and deal type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21fc495d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advanced M&A sentiment analyzer initialized\n",
      "I am retrieving M&A-relevant articles for sentiment analysis...\n",
      "I found 9 M&A-relevant articles to analyze\n",
      "I am analyzing sentiment for each article...\n",
      "I successfully analyzed sentiment for 9 articles\n",
      "I am saving sentiment analysis results to the database...\n",
      "I successfully updated 9 articles with sentiment data\n",
      "\n",
      "Most positive M&A sentiment articles:\n",
      "  1. [+0.84|0.84] SEC Charges Georgia-based First Liberty Building & Loan and its O... (SEC Press Releases)\n",
      "  2. [+0.75|0.75] Danantara and GEM to Develop $1.4 Billion Indonesia Nickel Plant... (Bloomberg M&A)\n",
      "  3. [+0.62|0.62] Billionaire Mouton’s Trust to Buy South African Schools Firm... (Bloomberg M&A)\n",
      "\n",
      "Most negative M&A sentiment articles:\n",
      "  1. [-0.86|0.86] Trump Slaps 50% Tariffs on India, Raising Tensions With Modi | Th... (Bloomberg M&A)\n",
      "  2. [-0.74|0.74] How Chris Pratt got dragged into Katy Perry’s legal battle with a... (MarketWatch)\n",
      "  3. [-0.36|0.36] China’s $1 Trillion Stock Rally Spurs Curbs From Broker, Funds... (Bloomberg M&A)\n",
      "  4. [-0.32|0.32] Founder and Owner of Washington-Based Water Machine Manufacturer ... (SEC Press Releases)\n",
      "\n",
      "Sentiment analysis by news source:\n",
      "  SEC Press Releases: +0.183 avg (positive, 3 articles, 33.3% positive)\n",
      "  Bloomberg M&A: +0.039 avg (neutral, 4 articles, 50.0% positive)\n",
      "  MarketWatch: -0.735 avg (negative, 1 articles, 0.0% positive)\n",
      "  Yahoo Finance: +0.000 avg (neutral, 1 articles, 0.0% positive)\n",
      "\n",
      "Overall M&A market sentiment analysis:\n",
      "Market sentiment: MIXED\n",
      "Sentiment strength: 0.004\n",
      "Articles analyzed: 9\n",
      "Positive articles: 3 (33.3%)\n",
      "Negative articles: 4 (44.4%)\n",
      "\n",
      "I am creating a sentiment tracking system for trend analysis...\n",
      "I recorded today's sentiment trends for historical tracking\n",
      "\n",
      "============================================================\n",
      "Sentiment Analysis Summary:\n",
      "Articles analyzed: 9\n",
      "Positive sentiment: 3 (33.3%)\n",
      "Negative sentiment: 4 (44.4%)\n",
      "Neutral sentiment: 2 (22.2%)\n",
      "High confidence analyses: 6 (66.7%)\n",
      "\n",
      "Database status:\n",
      "Articles with sentiment analysis: 9\n",
      "Average sentiment score: -0.004\n",
      "\n",
      "Sentiment analysis system is operational and tracking market sentiment\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# I will create a comprehensive sentiment analysis system\n",
    "class MASentimentAnalyzer:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        I am initializing the sentiment analysis system with multiple approaches\n",
    "        \"\"\"\n",
    "        self.vader_analyzer = SentimentIntensityAnalyzer()\n",
    "        \n",
    "        # M&A-specific sentiment modifiers\n",
    "        self.positive_ma_terms = {\n",
    "            'synergies': 0.3, 'strategic fit': 0.2, 'value creation': 0.3,\n",
    "            'complementary': 0.2, 'strengthen': 0.2, 'enhance': 0.2,\n",
    "            'accelerate growth': 0.3, 'market leader': 0.3, 'premium': 0.1,\n",
    "            'successful': 0.2, 'approved': 0.3, 'completed': 0.2\n",
    "        }\n",
    "        \n",
    "        self.negative_ma_terms = {\n",
    "            'regulatory concerns': -0.3, 'antitrust': -0.4, 'blocked': -0.5,\n",
    "            'failed': -0.4, 'terminated': -0.4, 'withdrawn': -0.3,\n",
    "            'hostile': -0.4, 'rejected': -0.4, 'opposition': -0.3,\n",
    "            'dilutive': -0.3, 'overpaid': -0.4, 'struggling': -0.3\n",
    "        }\n",
    "    \n",
    "    def analyze_sentiment(self, text):\n",
    "        \"\"\"\n",
    "        I will analyze sentiment using multiple methods and return comprehensive results\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return {\n",
    "                'compound_score': 0.0,\n",
    "                'positive': 0.0,\n",
    "                'negative': 0.0,\n",
    "                'neutral': 1.0,\n",
    "                'sentiment_label': 'neutral',\n",
    "                'confidence': 0.0,\n",
    "                'ma_adjusted_score': 0.0,\n",
    "                'analysis_method': 'vader'\n",
    "            }\n",
    "        \n",
    "        # VADER analysis\n",
    "        vader_scores = self.vader_analyzer.polarity_scores(text)\n",
    "        \n",
    "        # Apply M&A-specific adjustments\n",
    "        ma_adjustment = 0.0\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Positive M&A terms\n",
    "        for term, weight in self.positive_ma_terms.items():\n",
    "            if term in text_lower:\n",
    "                ma_adjustment += weight\n",
    "        \n",
    "        # Negative M&A terms\n",
    "        for term, weight in self.negative_ma_terms.items():\n",
    "            if term in text_lower:\n",
    "                ma_adjustment += weight  # weight is already negative\n",
    "        \n",
    "        # Calculate M&A-adjusted compound score\n",
    "        ma_adjusted_score = vader_scores['compound'] + ma_adjustment\n",
    "        ma_adjusted_score = max(-1.0, min(1.0, ma_adjusted_score))  # Keep in range [-1, 1]\n",
    "        \n",
    "        # Determine sentiment label based on adjusted score\n",
    "        if ma_adjusted_score >= 0.05:\n",
    "            sentiment_label = 'positive'\n",
    "            confidence = abs(ma_adjusted_score)\n",
    "        elif ma_adjusted_score <= -0.05:\n",
    "            sentiment_label = 'negative'  \n",
    "            confidence = abs(ma_adjusted_score)\n",
    "        else:\n",
    "            sentiment_label = 'neutral'\n",
    "            confidence = 1.0 - abs(ma_adjusted_score)\n",
    "        \n",
    "        return {\n",
    "            'compound_score': vader_scores['compound'],\n",
    "            'positive': vader_scores['pos'],\n",
    "            'negative': vader_scores['neg'],\n",
    "            'neutral': vader_scores['neu'],\n",
    "            'sentiment_label': sentiment_label,\n",
    "            'confidence': confidence,\n",
    "            'ma_adjusted_score': ma_adjusted_score,\n",
    "            'analysis_method': 'vader_ma_enhanced'\n",
    "        }\n",
    "    \n",
    "    def analyze_market_sentiment(self, articles_data):\n",
    "        \"\"\"\n",
    "        I will analyze overall market sentiment from multiple articles\n",
    "        \"\"\"\n",
    "        if not articles_data:\n",
    "            return {'overall_sentiment': 'neutral', 'sentiment_strength': 0.0, 'article_count': 0}\n",
    "        \n",
    "        sentiments = [article['ma_adjusted_score'] for article in articles_data]\n",
    "        \n",
    "        avg_sentiment = sum(sentiments) / len(sentiments)\n",
    "        positive_count = sum(1 for s in sentiments if s > 0.05)\n",
    "        negative_count = sum(1 for s in sentiments if s < -0.05)\n",
    "        \n",
    "        # Determine overall market sentiment\n",
    "        if avg_sentiment > 0.1:\n",
    "            overall_sentiment = 'bullish'\n",
    "        elif avg_sentiment < -0.1:\n",
    "            overall_sentiment = 'bearish'\n",
    "        else:\n",
    "            overall_sentiment = 'mixed'\n",
    "        \n",
    "        return {\n",
    "            'overall_sentiment': overall_sentiment,\n",
    "            'sentiment_strength': abs(avg_sentiment),\n",
    "            'article_count': len(articles_data),\n",
    "            'positive_articles': positive_count,\n",
    "            'negative_articles': negative_count,\n",
    "            'average_score': avg_sentiment\n",
    "        }\n",
    "\n",
    "# Initialize sentiment analyzer\n",
    "sentiment_analyzer = MASentimentAnalyzer()\n",
    "print(\"Advanced M&A sentiment analyzer initialized\")\n",
    "\n",
    "# I will analyze sentiment for all M&A-relevant articles\n",
    "print(\"I am retrieving M&A-relevant articles for sentiment analysis...\")\n",
    "\n",
    "cursor.execute('''\n",
    "    SELECT article_id, headline, summary, ma_relevance_score, source_name, published_date\n",
    "    FROM news_articles \n",
    "    WHERE ma_relevance_score > 0.3\n",
    "    ORDER BY ma_relevance_score DESC, published_date DESC\n",
    "''')\n",
    "\n",
    "articles_for_sentiment = cursor.fetchall()\n",
    "print(f\"I found {len(articles_for_sentiment)} M&A-relevant articles to analyze\")\n",
    "\n",
    "# Perform sentiment analysis\n",
    "sentiment_results = []\n",
    "analysis_progress = {\n",
    "    'total_articles': len(articles_for_sentiment),\n",
    "    'analyzed': 0,\n",
    "    'positive_sentiment': 0,\n",
    "    'negative_sentiment': 0,\n",
    "    'neutral_sentiment': 0,\n",
    "    'high_confidence': 0\n",
    "}\n",
    "\n",
    "print(\"I am analyzing sentiment for each article...\")\n",
    "\n",
    "for article_id, headline, summary, ma_score, source_name, pub_date in articles_for_sentiment:\n",
    "    try:\n",
    "        # Combine headline and summary for analysis\n",
    "        full_text = f\"{headline}. {summary or ''}\"\n",
    "        \n",
    "        # Analyze sentiment\n",
    "        sentiment_data = sentiment_analyzer.analyze_sentiment(full_text)\n",
    "        \n",
    "        # Store results\n",
    "        result = {\n",
    "            'article_id': article_id,\n",
    "            'headline': headline,\n",
    "            'ma_relevance_score': ma_score,\n",
    "            'source_name': source_name,\n",
    "            'published_date': pub_date,\n",
    "            **sentiment_data  # Unpack sentiment analysis results\n",
    "        }\n",
    "        \n",
    "        sentiment_results.append(result)\n",
    "        \n",
    "        # Update progress statistics\n",
    "        analysis_progress['analyzed'] += 1\n",
    "        \n",
    "        if sentiment_data['sentiment_label'] == 'positive':\n",
    "            analysis_progress['positive_sentiment'] += 1\n",
    "        elif sentiment_data['sentiment_label'] == 'negative':\n",
    "            analysis_progress['negative_sentiment'] += 1\n",
    "        else:\n",
    "            analysis_progress['neutral_sentiment'] += 1\n",
    "        \n",
    "        if sentiment_data['confidence'] > 0.7:\n",
    "            analysis_progress['high_confidence'] += 1\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing sentiment for article {article_id}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(f\"I successfully analyzed sentiment for {analysis_progress['analyzed']} articles\")\n",
    "\n",
    "# I will update the database with sentiment analysis results\n",
    "print(\"I am saving sentiment analysis results to the database...\")\n",
    "\n",
    "updates_completed = 0\n",
    "for result in sentiment_results:\n",
    "    try:\n",
    "        cursor.execute('''\n",
    "            UPDATE news_articles \n",
    "            SET sentiment_score = ?,\n",
    "                sentiment_label = ?,\n",
    "                confidence_score = ?\n",
    "            WHERE article_id = ?\n",
    "        ''', (\n",
    "            result['ma_adjusted_score'],\n",
    "            result['sentiment_label'],\n",
    "            result['confidence'],\n",
    "            result['article_id']\n",
    "        ))\n",
    "        \n",
    "        if cursor.rowcount > 0:\n",
    "            updates_completed += 1\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error updating sentiment for article {result['article_id']}: {str(e)}\")\n",
    "\n",
    "db_connection.commit()\n",
    "print(f\"I successfully updated {updates_completed} articles with sentiment data\")\n",
    "\n",
    "# I want to show the most interesting sentiment results\n",
    "print(\"\\nMost positive M&A sentiment articles:\")\n",
    "positive_articles = [r for r in sentiment_results if r['sentiment_label'] == 'positive']\n",
    "positive_articles.sort(key=lambda x: x['ma_adjusted_score'], reverse=True)\n",
    "\n",
    "for i, article in enumerate(positive_articles[:5]):\n",
    "    score = article['ma_adjusted_score']\n",
    "    confidence = article['confidence']\n",
    "    headline = article['headline'][:65]\n",
    "    source = article['source_name']\n",
    "    print(f\"  {i+1}. [+{score:.2f}|{confidence:.2f}] {headline}... ({source})\")\n",
    "\n",
    "print(\"\\nMost negative M&A sentiment articles:\")\n",
    "negative_articles = [r for r in sentiment_results if r['sentiment_label'] == 'negative']\n",
    "negative_articles.sort(key=lambda x: x['ma_adjusted_score'])\n",
    "\n",
    "for i, article in enumerate(negative_articles[:5]):\n",
    "    score = article['ma_adjusted_score']\n",
    "    confidence = article['confidence']\n",
    "    headline = article['headline'][:65]\n",
    "    source = article['source_name']\n",
    "    print(f\"  {i+1}. [{score:.2f}|{confidence:.2f}] {headline}... ({source})\")\n",
    "\n",
    "# I will analyze sentiment patterns by source\n",
    "print(\"\\nSentiment analysis by news source:\")\n",
    "source_sentiment = defaultdict(list)\n",
    "\n",
    "for result in sentiment_results:\n",
    "    source_sentiment[result['source_name']].append(result['ma_adjusted_score'])\n",
    "\n",
    "for source, scores in source_sentiment.items():\n",
    "    if scores:\n",
    "        avg_sentiment = sum(scores) / len(scores)\n",
    "        article_count = len(scores)\n",
    "        positive_ratio = sum(1 for s in scores if s > 0.05) / article_count\n",
    "        \n",
    "        sentiment_desc = \"positive\" if avg_sentiment > 0.05 else \"negative\" if avg_sentiment < -0.05 else \"neutral\"\n",
    "        print(f\"  {source}: {avg_sentiment:+.3f} avg ({sentiment_desc}, {article_count} articles, {positive_ratio:.1%} positive)\")\n",
    "\n",
    "# I will create a market sentiment summary\n",
    "market_analysis = sentiment_analyzer.analyze_market_sentiment(sentiment_results)\n",
    "\n",
    "print(f\"\\nOverall M&A market sentiment analysis:\")\n",
    "print(f\"Market sentiment: {market_analysis['overall_sentiment'].upper()}\")\n",
    "print(f\"Sentiment strength: {market_analysis['sentiment_strength']:.3f}\")\n",
    "print(f\"Articles analyzed: {market_analysis['article_count']}\")\n",
    "print(f\"Positive articles: {market_analysis['positive_articles']} ({market_analysis['positive_articles']/market_analysis['article_count']:.1%})\")\n",
    "print(f\"Negative articles: {market_analysis['negative_articles']} ({market_analysis['negative_articles']/market_analysis['article_count']:.1%})\")\n",
    "\n",
    "# I will create a sentiment tracking table for historical analysis\n",
    "print(\"\\nI am creating a sentiment tracking system for trend analysis...\")\n",
    "\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS sentiment_trends (\n",
    "    trend_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    analysis_date DATE NOT NULL,\n",
    "    \n",
    "    -- Aggregate sentiment metrics\n",
    "    total_articles_analyzed INTEGER,\n",
    "    average_sentiment REAL,\n",
    "    positive_articles INTEGER,\n",
    "    negative_articles INTEGER,\n",
    "    neutral_articles INTEGER,\n",
    "    \n",
    "    -- Market classification\n",
    "    market_sentiment VARCHAR(20),  -- 'bullish', 'bearish', 'mixed'\n",
    "    sentiment_strength REAL,\n",
    "    \n",
    "    -- Source breakdown (JSON)\n",
    "    source_breakdown TEXT,\n",
    "    \n",
    "    created_at DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    ")\n",
    "''')\n",
    "\n",
    "# Insert today's sentiment trend\n",
    "today_date = datetime.now().date()\n",
    "source_breakdown_json = json.dumps({\n",
    "    source: {\n",
    "        'avg_sentiment': sum(scores) / len(scores),\n",
    "        'article_count': len(scores)\n",
    "    }\n",
    "    for source, scores in source_sentiment.items() if scores\n",
    "})\n",
    "\n",
    "cursor.execute('''\n",
    "    INSERT OR REPLACE INTO sentiment_trends \n",
    "    (analysis_date, total_articles_analyzed, average_sentiment,\n",
    "     positive_articles, negative_articles, neutral_articles,\n",
    "     market_sentiment, sentiment_strength, source_breakdown)\n",
    "    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "''', (\n",
    "    today_date,\n",
    "    analysis_progress['analyzed'],\n",
    "    market_analysis['average_score'],\n",
    "    analysis_progress['positive_sentiment'],\n",
    "    analysis_progress['negative_sentiment'], \n",
    "    analysis_progress['neutral_sentiment'],\n",
    "    market_analysis['overall_sentiment'],\n",
    "    market_analysis['sentiment_strength'],\n",
    "    source_breakdown_json\n",
    "))\n",
    "\n",
    "db_connection.commit()\n",
    "print(f\"I recorded today's sentiment trends for historical tracking\")\n",
    "\n",
    "# Final statistics summary\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"Sentiment Analysis Summary:\")\n",
    "print(f\"Articles analyzed: {analysis_progress['analyzed']}\")\n",
    "print(f\"Positive sentiment: {analysis_progress['positive_sentiment']} ({analysis_progress['positive_sentiment']/analysis_progress['analyzed']:.1%})\")\n",
    "print(f\"Negative sentiment: {analysis_progress['negative_sentiment']} ({analysis_progress['negative_sentiment']/analysis_progress['analyzed']:.1%})\")\n",
    "print(f\"Neutral sentiment: {analysis_progress['neutral_sentiment']} ({analysis_progress['neutral_sentiment']/analysis_progress['analyzed']:.1%})\")\n",
    "print(f\"High confidence analyses: {analysis_progress['high_confidence']} ({analysis_progress['high_confidence']/analysis_progress['analyzed']:.1%})\")\n",
    "\n",
    "# I want to show database status after sentiment analysis\n",
    "cursor.execute('SELECT COUNT(*) FROM news_articles WHERE sentiment_score IS NOT NULL')\n",
    "articles_with_sentiment = cursor.fetchone()[0]\n",
    "\n",
    "cursor.execute('SELECT AVG(sentiment_score) FROM news_articles WHERE sentiment_score IS NOT NULL')\n",
    "avg_sentiment_db = cursor.fetchone()[0] or 0\n",
    "\n",
    "print(f\"\\nDatabase status:\")\n",
    "print(f\"Articles with sentiment analysis: {articles_with_sentiment}\")\n",
    "print(f\"Average sentiment score: {avg_sentiment_db:+.3f}\")\n",
    "\n",
    "print(f\"\\nSentiment analysis system is operational and tracking market sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2967b13d",
   "metadata": {},
   "source": [
    "### Historical Data Scraping\n",
    "\n",
    "**I will systematically collect major M&A deals and related news articles from the last 5 years to create a professional-grade historical dataset. This transforms our system from having limited current data to having comprehensive market intelligence spanning multiple M&A cycles.**\n",
    "\n",
    "### Tools being used:\n",
    "\n",
    "- Wikipedia scraping: Systematic collection of major deals by year\n",
    "- Google News search: Historical article collection for major transactions\n",
    "- SEC EDGAR: Historical filings for public company M&A activity\n",
    "- Data validation: Cross-referencing and quality control across sources"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
